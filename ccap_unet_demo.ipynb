{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aml7hawaiiedu/CCAPLandCoverProject/blob/main/ccap_unet_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6-_rOVHH3if"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "This is Amanda's version of UNET_regression_demo_whp (Aron's original code)\n",
        "  \n",
        "This is an Earth Engine <> TenserFlow demonstration notebook. Suppose you want to predict a continuous output (regression) from a stack of continuous inputs. In this example, the output is impervious surface area from [NLCD](https://www.mrlc.gov/data) and the input is a Landsat 8 composite. The model is a [fully convolutional neural network  (FCNN)](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf), specifically [U-net](https://arxiv.org/abs/1505.04597). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "okAzQzK0J4HS"
      },
      "outputs": [],
      "source": [
        "# Google Cloud Platform authentication.\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXQd9K91KLIx",
        "outputId": "5443a67d-5dff-40ac-b5e1-00d8aab8f036"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To authorize access needed by Earth Engine, open the following URL in a web browser and follow the instructions. If the web browser does not start automatically, please manually browse the URL below.\n",
            "\n",
            "    https://code.earthengine.google.com/client-auth?scopes=https%3A//www.googleapis.com/auth/earthengine%20https%3A//www.googleapis.com/auth/devstorage.full_control&request_id=Ez6V6ttzimfNb1uJK-OxxXIE-k-A7oqsMB5KVd0RTBk&tc=tD6PPIPzq9pjyUprmLqrZloY_e_kkR6FV_WkYV6cQCI&cc=-swdgakM-lwMveg1GcJfmlxpOSV0QornbG6qaADRvvg\n",
            "\n",
            "The authorization workflow will generate a code, which you should paste in the box below.\n",
            "Enter verification code: 4/1AVHEtk5Srzrz-chT93rx3hghgbhow5xOaESQBmGuVUMKHJIaz9uiPa35vWc\n",
            "\n",
            "Successfully saved authorization token.\n"
          ]
        }
      ],
      "source": [
        "# Import Google Earth Engine Python API, authenticate user's credentials, and initialize the Earth Engine library.\n",
        "import ee\n",
        "ee.Authenticate()\n",
        "ee.Initialize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-N6SCzKDKc5g",
        "outputId": "7cddcba6-207f-4c34-b408-0901bfbd7b89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.12.0\n"
          ]
        }
      ],
      "source": [
        "# Tensorflow is an open-source machine learning library developed by Google. It is wildly used for building deep learning models.\n",
        "# Print the version.\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3qKGJ7evWG7",
        "outputId": "490a6419-6545-4fe8-c6d5-276d50e8aad4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.14.0\n"
          ]
        }
      ],
      "source": [
        "# Folium is a Python library used for creating interactive maps and visualizations.\n",
        "# Print the version. \n",
        "import folium\n",
        "print(folium.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "cGuJGj71zX2a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fd094a4-c451-40e8-f450-d3a5eaa63e2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "remote_sensing_fuckit_bucket\n"
          ]
        }
      ],
      "source": [
        "# Name of the Google Cloud Storage (GCS) bucket where data will be stored or accessed from.\n",
        "# GCS is cloud-based object storage service used to store and access large amounts of data in a highly scalable and durable manner. \n",
        "# It is common to define a bucket which serves as a top-level container. \n",
        "BUCKET = 'remote_sensing_fuckit_bucket'\n",
        "print(BUCKET)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "bWSU2lVyzYVI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6046a0b1-939a-4933-befd-25867a7969f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b1\n"
          ]
        }
      ],
      "source": [
        "# Name of the folder that will be used to store data related to the wetland U-Net model.\n",
        "# Name of the subfolder within 'FOLDER' directory that will be used to store training patches or data used to train the wetland classification model. \n",
        "# Name of the subfolder within 'FOLDER' directory that will be used to store evaluation patches or data used to test the wetland classification model. \n",
        "FOLDER = 'wetland_unet'\n",
        "TRAINING_BASE = 'training_patches'\n",
        "EVAL_BASE = 'eval_patches'\n",
        "\n",
        "# These lines of code specify the input features for the wetland U-Net model and the response variable that the model will be trained to predict.\n",
        "# opticalBands is assigned as a list of string values, which correspond to the names of the optical bands that are present in the Landsat satellite imagery.\n",
        "# thermalBands is assigned a list of string values, which correspond to the names of the optical bands that are present in the Landsat satellite imagery.\n",
        "# BANDS is assigned the concatenation of the 'opticalBands' and 'thermalBands'. This represents a list of all the bands that will be used as features in the U-Net model.\n",
        "# RESPONSE is assigned the string value 'b1' which corresponds to the name of the response variable that the U-Net model will be trained to predict. \n",
        "# FEATURES is assigned the concatenation of the 'BANDS' list and the 'RESPONSE' variables. This represents a list of all the features that will be used to train the U-Net model. \n",
        "opticalBands = ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7']\n",
        "thermalBands = ['B10', 'B11']\n",
        "BANDS = opticalBands + thermalBands\n",
        "RESPONSE = 'b1'\n",
        "FEATURES = BANDS + [RESPONSE]\n",
        "\n",
        "# Specify the inputs\n",
        "# These lines of code specify the size and shape of patches that the U-Net model expects as input, as well as a dictionary that maps featue names to columns in an TenserFlow dataset. \n",
        "# KERNEL_SIZE is assigned the integer value '128' which represents the size of patches that the U-Net model expects as input.\n",
        "# KERNEL_SHAPE is assigned a list contning two elements, both of which are equal to 'KERNEL_SIZE'. This represents the shape of the patches the U-Net model expects as an input.\n",
        "# COLUMNS is assigned a list of TenserFlow 'fixedLenFeature' objects, which specify the expected shape and data type of each feature column in a Tenserflow dataset. \n",
        "  # 'shape' is set to 'KERNEL_SHAPE' and 'dtype' is set to 'tf.float32'. The list comprehension used here creates one 'fixedLenFeature' object for each feature in the 'FEATURES' list/ \n",
        "# FEATURE_DICT is assigned a Python dictionary that maps feature names to their corresponding columns in a TenserFlow dataset. \n",
        "  # 'zip' is used to create tuples parining each feature name from the 'FEATURES' list with its corresponding 'fixedLenFeature' object from the 'COLUMNS' list. \n",
        "  # The 'dict' function is then used to convert this list of tuples into a dictionary. \n",
        "KERNEL_SIZE = 128\n",
        "KERNEL_SHAPE = [KERNEL_SIZE, KERNEL_SIZE]\n",
        "COLUMNS = [\n",
        "  tf.io.FixedLenFeature(shape=KERNEL_SHAPE, dtype=tf.float32) for k in FEATURES\n",
        "]\n",
        "FEATURES_DICT = dict(zip(FEATURES, COLUMNS))\n",
        "\n",
        "# Define the size of the training and evaluation datasets that will be used to train and evaluate the U-Net model. \n",
        "# TRAIN_SIZE is assigned the integer value '1600' which represents the size of the training dataset.\n",
        "# EVAL_SIZE is assigned the integer value '800' which represents the size of the evaluation dataset. \n",
        "TRAIN_SIZE = 1600\n",
        "EVAL_SIZE = 800\n",
        "\n",
        "# Specify model training parameters.\n",
        "# Define model training parameters that will be used to train and evaluate the U-Net model. \n",
        "# BATCH_SIZE represents the number of training examples that will be used in each training iteration. \n",
        "# EPOCHS represents the number of times that each training dataset will be used to train the model. \n",
        "# BUFFER_SIZE represents the number of elements from the training dataset that will be pre-fetched and buffered in order to optimize data reading during training. \n",
        "# OPTIMIZER specifies the Stochastic Gradient Descent optimization algorithm will be used during training. \n",
        "  # ? what other optimizers would work? Is this the best one for this type of model?\n",
        "# LOSS specifies the Mean Squared Error Loss function will be used during training. \n",
        "# METRICS is asigned a list containing the sting value 'RootMeanSquareError' which specifies RMSE metric will be used to evaluate the performance of the model during training. \n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 10\n",
        "BUFFER_SIZE = 2000\n",
        "OPTIMIZER = 'Adam'\n",
        "LOSS = 'CategoricalCrossentropy'\n",
        "METRICS = ['RootMeanSquaredError']\n",
        "\n",
        "print(RESPONSE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 796
        },
        "id": "tnLLLqcmzeVR",
        "outputId": "4992e103-fa11-4784-8270-ef0890f0d47e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<folium.folium.Map at 0x7f64ed881190>"
            ],
            "text/html": [
              "<div style=\"width:100%;\"><div style=\"position:relative;width:100%;height:0;padding-bottom:60%;\"><span style=\"color:#565656\">Make this Notebook Trusted to load map: File -> Trust Notebook</span><iframe srcdoc=\"&lt;!DOCTYPE html&gt;\n",
              "&lt;html&gt;\n",
              "&lt;head&gt;\n",
              "    \n",
              "    &lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html; charset=UTF-8&quot; /&gt;\n",
              "    \n",
              "        &lt;script&gt;\n",
              "            L_NO_TOUCH = false;\n",
              "            L_DISABLE_3D = false;\n",
              "        &lt;/script&gt;\n",
              "    \n",
              "    &lt;style&gt;html, body {width: 100%;height: 100%;margin: 0;padding: 0;}&lt;/style&gt;\n",
              "    &lt;style&gt;#map {position:absolute;top:0;bottom:0;right:0;left:0;}&lt;/style&gt;\n",
              "    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.js&quot;&gt;&lt;/script&gt;\n",
              "    &lt;script src=&quot;https://code.jquery.com/jquery-1.12.4.min.js&quot;&gt;&lt;/script&gt;\n",
              "    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/js/bootstrap.bundle.min.js&quot;&gt;&lt;/script&gt;\n",
              "    &lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.js&quot;&gt;&lt;/script&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/css/bootstrap.min.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://netdna.bootstrapcdn.com/bootstrap/3.0.0/css/bootstrap.min.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.2.0/css/all.min.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/gh/python-visualization/folium/folium/templates/leaflet.awesome.rotate.min.css&quot;/&gt;\n",
              "    \n",
              "            &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width,\n",
              "                initial-scale=1.0, maximum-scale=1.0, user-scalable=no&quot; /&gt;\n",
              "            &lt;style&gt;\n",
              "                #map_4bee2a04625ec2618270d87326006ab2 {\n",
              "                    position: relative;\n",
              "                    width: 100.0%;\n",
              "                    height: 100.0%;\n",
              "                    left: 0.0%;\n",
              "                    top: 0.0%;\n",
              "                }\n",
              "                .leaflet-container { font-size: 1rem; }\n",
              "            &lt;/style&gt;\n",
              "        \n",
              "&lt;/head&gt;\n",
              "&lt;body&gt;\n",
              "    \n",
              "    \n",
              "            &lt;div class=&quot;folium-map&quot; id=&quot;map_4bee2a04625ec2618270d87326006ab2&quot; &gt;&lt;/div&gt;\n",
              "        \n",
              "&lt;/body&gt;\n",
              "&lt;script&gt;\n",
              "    \n",
              "    \n",
              "            var map_4bee2a04625ec2618270d87326006ab2 = L.map(\n",
              "                &quot;map_4bee2a04625ec2618270d87326006ab2&quot;,\n",
              "                {\n",
              "                    center: [21.4, -158.0],\n",
              "                    crs: L.CRS.EPSG3857,\n",
              "                    zoom: 10,\n",
              "                    zoomControl: true,\n",
              "                    preferCanvas: false,\n",
              "                }\n",
              "            );\n",
              "\n",
              "            \n",
              "\n",
              "        \n",
              "    \n",
              "            var tile_layer_7c49d5d3ef3b76a916551ec965a089ed = L.tileLayer(\n",
              "                &quot;https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png&quot;,\n",
              "                {&quot;attribution&quot;: &quot;Data by \\u0026copy; \\u003ca target=\\&quot;_blank\\&quot; href=\\&quot;http://openstreetmap.org\\&quot;\\u003eOpenStreetMap\\u003c/a\\u003e, under \\u003ca target=\\&quot;_blank\\&quot; href=\\&quot;http://www.openstreetmap.org/copyright\\&quot;\\u003eODbL\\u003c/a\\u003e.&quot;, &quot;detectRetina&quot;: false, &quot;maxNativeZoom&quot;: 18, &quot;maxZoom&quot;: 18, &quot;minZoom&quot;: 0, &quot;noWrap&quot;: false, &quot;opacity&quot;: 1, &quot;subdomains&quot;: &quot;abc&quot;, &quot;tms&quot;: false}\n",
              "            ).addTo(map_4bee2a04625ec2618270d87326006ab2);\n",
              "        \n",
              "    \n",
              "            var tile_layer_691d40629da04c2ca01c4d1e0587531d = L.tileLayer(\n",
              "                &quot;https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/maps/4f43207683f027a6ff40a2bf98d63dcc-bf43be14d400d5e0779a7327b6dc36f2/tiles/{z}/{x}/{y}&quot;,\n",
              "                {&quot;attribution&quot;: &quot;Map Data \\u0026copy; \\u003ca href=\\&quot;https://earthengine.google.com/\\&quot;\\u003eGoogle Earth Engine\\u003c/a\\u003e&quot;, &quot;detectRetina&quot;: false, &quot;maxNativeZoom&quot;: 18, &quot;maxZoom&quot;: 18, &quot;minZoom&quot;: 0, &quot;noWrap&quot;: false, &quot;opacity&quot;: 1, &quot;subdomains&quot;: &quot;abc&quot;, &quot;tms&quot;: false}\n",
              "            ).addTo(map_4bee2a04625ec2618270d87326006ab2);\n",
              "        \n",
              "&lt;/script&gt;\n",
              "&lt;/html&gt;\" style=\"position:absolute;width:100%;height:100%;left:0;top:0;border:none !important;\" allowfullscreen webkitallowfullscreen mozallowfullscreen></iframe></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "# Use Landsat 8 surface reflectance data.\n",
        "# Use EE Python API to initialize an image collection. \n",
        "# Specify Landsat 8 surface reflectance image collection. \n",
        "# l8sr is the name of the object to be used in subsequent operations.\n",
        "\n",
        "l8sr = ee.ImageCollection('LANDSAT/LC08/C01/T1_SR')\n",
        "\n",
        "\n",
        "# This code defines a function 'maskL8sr' that will be used to apply cloud masking to Landsat 8 surface reflectance images.\n",
        "def maskL8sr(image):\n",
        "  cloudShadowBitMask = ee.Number(2).pow(3).int()\n",
        "  cloudsBitMask = ee.Number(2).pow(5).int()\n",
        "  qa = image.select('pixel_qa')\n",
        "  mask1 = qa.bitwiseAnd(cloudShadowBitMask).eq(0).And(\n",
        "    qa.bitwiseAnd(cloudsBitMask).eq(0))\n",
        "  mask2 = image.mask().reduce('min')\n",
        "  mask3 = image.select(opticalBands).gt(0).And(\n",
        "          image.select(opticalBands).lt(10000)).reduce('min')\n",
        "  mask = mask1.And(mask2).And(mask3)\n",
        "  return image.select(opticalBands).divide(10000).addBands(\n",
        "          image.select(thermalBands).divide(10).clamp(273.15, 373.15)\n",
        "            .subtract(273.15).divide(100)).updateMask(mask)\n",
        "\n",
        "\n",
        "\n",
        "# The image input data is a cloud-masked median composite.\n",
        "# Filter the l8sr image collection by date, selecting only images from the date range identified. \n",
        "# Apply the mask function to each image in the resulting collection using the map() method. \n",
        "  # This function applies a mask to each image and rescales the reflectance and temperature bands to a 0-1 range. \n",
        "# Calculate the median value of the resulting image collection using the median() method.\n",
        "# Assign the variable name; in this case 'ls_2017'. \n",
        "# The resulting 'ls_2017' image will be used for training and evaluation of our U-Net model. \n",
        "\n",
        "ls_2017 = l8sr.filterDate('2017-01-01', '2017-12-30').map(maskL8sr).median()\n",
        "\n",
        "\n",
        "# Create single mosaic\n",
        "#ls_3yr_mosaic =  ls_2017.addBands(ls_2018)\n",
        "#ls_3yr_mosaic = ls_3yr_mosaic.addBands(ls_2019)\n",
        "\n",
        "#Reset band names to new stack\n",
        "#BANDS = ls_3yr_mosaic.bandNames().getInfo()\n",
        "#FEATURES = BANDS + [RESPONSE]\n",
        "\n",
        "# Use folium to visualize the imagery.\n",
        "# map = folium.Map(location=[21.4, -158.])\n",
        "\n",
        "\n",
        "\n",
        "# Use 'getMapID()' method to get a map ID for the 'ls_2017' image. \n",
        "# Specify that we want to visualize band 2 of the image using 'bands':['B2']. Set the min and max.\n",
        "# Create a folium map object centered at specific location using 'folium.Map()'. \n",
        "# Add a tile layer to the map using 'folium.TileLayer()' method, specifying the URL format of the tiles using the 'tiles' argument and adding 'attr' attribution info. \n",
        "# Set the overlay flag to 'True' and provide a name for the layer using 'name'. \n",
        "# Finally, add the tile layer to the map using '.add_to()' method. \n",
        "  # This allows us to visualize the 'ls_2017' image in the folium map. \n",
        "mapid_17 =ls_2017.getMapId({'bands': ['B2'], 'min': 0, 'max': 0.3})\n",
        "map = folium.Map(location=[21.4, -158.])\n",
        "folium.TileLayer(\n",
        "    tiles=mapid_17['tile_fetcher'].url_format,\n",
        "    attr = 'Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name = '17 median composite',\n",
        "    ).add_to(map)\n",
        "\n",
        "map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Slxl78VA2J8A"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "I4X7EyYwv6m0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b312533-dada-4c9c-c99d-746f86265306"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<function oneHot at 0x7f64ed94dee0>\n"
          ]
        }
      ],
      "source": [
        "# One Hot Encoding function.\n",
        "# 'oneHot()' function is used for One Hot Encoding of an image. \n",
        "  # Each pixel in the image is replaced by a binary vector of length 'n' where 'n' is the number of classes in the image. \n",
        "# image : image to be encoded\n",
        "# code : the class code that will be encoded as '1' in the output, while all other classes are encoded as '0'\n",
        "# The function compares each pixel value in the input 'image' with the 'code' value using the 'ee.image.eq()' finction. \n",
        "# The 'ee.image.eq()' function returns a binary image with the same dimensions as the input image, where each pixel is '1' if the input pixel is equal to the given 'code' value and '0' otherwise. \n",
        "# The resulting binary image is returned as the 'one_hot_image' encoded. \n",
        "def oneHot(image,code = 1):\n",
        "  one_hot_image = image.eq(code)\n",
        "  return one_hot_image\n",
        "  \n",
        "print(oneHot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 796
        },
        "id": "KxvKSAGh_7D3",
        "outputId": "55ce5513-6e47-4934-8c3d-0b4ec003995a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<folium.folium.Map at 0x7f64ec07b190>"
            ],
            "text/html": [
              "<div style=\"width:100%;\"><div style=\"position:relative;width:100%;height:0;padding-bottom:60%;\"><span style=\"color:#565656\">Make this Notebook Trusted to load map: File -> Trust Notebook</span><iframe srcdoc=\"&lt;!DOCTYPE html&gt;\n",
              "&lt;html&gt;\n",
              "&lt;head&gt;\n",
              "    \n",
              "    &lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html; charset=UTF-8&quot; /&gt;\n",
              "    \n",
              "        &lt;script&gt;\n",
              "            L_NO_TOUCH = false;\n",
              "            L_DISABLE_3D = false;\n",
              "        &lt;/script&gt;\n",
              "    \n",
              "    &lt;style&gt;html, body {width: 100%;height: 100%;margin: 0;padding: 0;}&lt;/style&gt;\n",
              "    &lt;style&gt;#map {position:absolute;top:0;bottom:0;right:0;left:0;}&lt;/style&gt;\n",
              "    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.js&quot;&gt;&lt;/script&gt;\n",
              "    &lt;script src=&quot;https://code.jquery.com/jquery-1.12.4.min.js&quot;&gt;&lt;/script&gt;\n",
              "    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/js/bootstrap.bundle.min.js&quot;&gt;&lt;/script&gt;\n",
              "    &lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.js&quot;&gt;&lt;/script&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/css/bootstrap.min.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://netdna.bootstrapcdn.com/bootstrap/3.0.0/css/bootstrap.min.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.2.0/css/all.min.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/gh/python-visualization/folium/folium/templates/leaflet.awesome.rotate.min.css&quot;/&gt;\n",
              "    \n",
              "            &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width,\n",
              "                initial-scale=1.0, maximum-scale=1.0, user-scalable=no&quot; /&gt;\n",
              "            &lt;style&gt;\n",
              "                #map_891cc22aee759c9ec725d91cd04ca1f3 {\n",
              "                    position: relative;\n",
              "                    width: 100.0%;\n",
              "                    height: 100.0%;\n",
              "                    left: 0.0%;\n",
              "                    top: 0.0%;\n",
              "                }\n",
              "                .leaflet-container { font-size: 1rem; }\n",
              "            &lt;/style&gt;\n",
              "        \n",
              "&lt;/head&gt;\n",
              "&lt;body&gt;\n",
              "    \n",
              "    \n",
              "            &lt;div class=&quot;folium-map&quot; id=&quot;map_891cc22aee759c9ec725d91cd04ca1f3&quot; &gt;&lt;/div&gt;\n",
              "        \n",
              "&lt;/body&gt;\n",
              "&lt;script&gt;\n",
              "    \n",
              "    \n",
              "            var map_891cc22aee759c9ec725d91cd04ca1f3 = L.map(\n",
              "                &quot;map_891cc22aee759c9ec725d91cd04ca1f3&quot;,\n",
              "                {\n",
              "                    center: [21.4, -158.0],\n",
              "                    crs: L.CRS.EPSG3857,\n",
              "                    zoom: 10,\n",
              "                    zoomControl: true,\n",
              "                    preferCanvas: false,\n",
              "                }\n",
              "            );\n",
              "\n",
              "            \n",
              "\n",
              "        \n",
              "    \n",
              "            var tile_layer_15071f481165f706a23d147cf901251f = L.tileLayer(\n",
              "                &quot;https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png&quot;,\n",
              "                {&quot;attribution&quot;: &quot;Data by \\u0026copy; \\u003ca target=\\&quot;_blank\\&quot; href=\\&quot;http://openstreetmap.org\\&quot;\\u003eOpenStreetMap\\u003c/a\\u003e, under \\u003ca target=\\&quot;_blank\\&quot; href=\\&quot;http://www.openstreetmap.org/copyright\\&quot;\\u003eODbL\\u003c/a\\u003e.&quot;, &quot;detectRetina&quot;: false, &quot;maxNativeZoom&quot;: 18, &quot;maxZoom&quot;: 18, &quot;minZoom&quot;: 0, &quot;noWrap&quot;: false, &quot;opacity&quot;: 1, &quot;subdomains&quot;: &quot;abc&quot;, &quot;tms&quot;: false}\n",
              "            ).addTo(map_891cc22aee759c9ec725d91cd04ca1f3);\n",
              "        \n",
              "    \n",
              "            var tile_layer_56724500202763724087bd13f189afe9 = L.tileLayer(\n",
              "                &quot;https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/maps/6f6172a7708cb45bbd815428fd6e8ce5-840d0e50d83481fbd6a7afcb543d6107/tiles/{z}/{x}/{y}&quot;,\n",
              "                {&quot;attribution&quot;: &quot;Map Data \\u0026copy; \\u003ca href=\\&quot;https://earthengine.google.com/\\&quot;\\u003eGoogle Earth Engine\\u003c/a\\u003e&quot;, &quot;detectRetina&quot;: false, &quot;maxNativeZoom&quot;: 18, &quot;maxZoom&quot;: 18, &quot;minZoom&quot;: 0, &quot;noWrap&quot;: false, &quot;opacity&quot;: 1, &quot;subdomains&quot;: &quot;abc&quot;, &quot;tms&quot;: false}\n",
              "            ).addTo(map_891cc22aee759c9ec725d91cd04ca1f3);\n",
              "        \n",
              "    \n",
              "            var layer_control_f02f8e1305cdeb3e4e87d03f5ce15eed = {\n",
              "                base_layers : {\n",
              "                    &quot;openstreetmap&quot; : tile_layer_15071f481165f706a23d147cf901251f,\n",
              "                },\n",
              "                overlays :  {\n",
              "                    &quot;NOAA CCAP Land Cover&quot; : tile_layer_56724500202763724087bd13f189afe9,\n",
              "                },\n",
              "            };\n",
              "            L.control.layers(\n",
              "                layer_control_f02f8e1305cdeb3e4e87d03f5ce15eed.base_layers,\n",
              "                layer_control_f02f8e1305cdeb3e4e87d03f5ce15eed.overlays,\n",
              "                {&quot;autoZIndex&quot;: true, &quot;collapsed&quot;: true, &quot;position&quot;: &quot;topright&quot;}\n",
              "            ).addTo(map_891cc22aee759c9ec725d91cd04ca1f3);\n",
              "        \n",
              "&lt;/script&gt;\n",
              "&lt;/html&gt;\" style=\"position:absolute;width:100%;height:100%;left:0;top:0;border:none !important;\" allowfullscreen webkitallowfullscreen mozallowfullscreen></iframe></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "# get imagecollection from google earth engine - NOAA C-CAP land cover data for Hawaii \n",
        "# This code gets an EE image collection which contains the land cover data for Hawaii form the NOAA Coastal Change Analysis Program (C-CAP)\n",
        "wetland = ee.ImageCollection(\"projects/sat-io/open-datasets/HRLC/CCAP_HI_LC\")\n",
        "\n",
        "# Filter image collection to include images within a specified date range. \n",
        "# The 'mosaic()' function is used to combine all the images in the filtered collection into a single image, which represents the most recent pixel value at each location. \n",
        "# This is done to reduce the numer of images in the collection and to create a single image for analysis. \n",
        "wetland = wetland.filterDate('2010-01-01', '2022-01-01').mosaic()\n",
        "\n",
        "# Mask out pixels with a value of 0 in the 'wetland' image using the mask() function.\n",
        "# This ensures the modeld doesn't predict or train on data outside of the acutual land cover area. \n",
        "wetland = wetland.mask(wetland.gt(0))\n",
        "\n",
        "# This generates a one-hot encoding representation of the land cover classification image for Hawaii obtained from the NOAA C-CAP dataset.\n",
        "# For each of the 25 land cover classes present in the image, the code creates a binary image where pixels with the corresponding class are set to 1 and pixels with other classes are set to 0.\n",
        "# The reutning binary iamges are stored in a list 'wetland_oneHot'.\n",
        "# The list is converted to a single multi-band image.\n",
        "# The bands of the multi-image are renamed to correspond to the class lables.\n",
        "wetland_oneHot = []\n",
        "for code in [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25]:\n",
        "  one_hot_image = oneHot(wetland,code)\n",
        "  wetland_oneHot.append(one_hot_image)\n",
        "wetland_oneHot = ee.Image(wetland_oneHot)\n",
        "wetland_oneHot = wetland_oneHot.rename(['p1','p2','p3','p4','p5','p6','p7','p8','p9','p10','p11','p12','p13','p14','p15','p16','p17','p18','p19','p20','p21','p22','p23','p24','p25'])\n",
        "\n",
        "# Use folium to visualize the imagery.\n",
        "# The 'wetland_oneHot' image is used to generate the map ID. \n",
        "# The min and max are set to 0 and 1 to display the binary values of the one-hot encoding. \n",
        "mapid = wetland_oneHot.getMapId({'bands': ['p15','p10','p2'],'min': 0, 'max': 1})\n",
        "map = folium.Map(location=[21.4, -158])\n",
        "folium.TileLayer(\n",
        "    tiles=mapid['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='NOAA CCAP Land Cover',\n",
        "  ).add_to(map)\n",
        "map.add_child(folium.LayerControl())\n",
        "map\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEk2QDx1_66B"
      },
      "source": [
        "Stack the 2D images (Landsat  and NOAA C-CAP land cover data for Hawaii) to create a single image from which samples can be taken. \n",
        "\n",
        "Convert the image into an array image in which each pixel stores 128x128 patches of pixels for each band. \n",
        "\n",
        "This is a key step that bears emphasis: to export training patches, convert a multi-band image to an array image using neighborhoodToArray(), then sample the image at points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozHkvbb76hv9",
        "outputId": "aa352754-48f1-48af-a231-609dd7aebd2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['p1', 'p2', 'p3', 'p4', 'p5', 'p6', 'p7', 'p8', 'p9', 'p10', 'p11', 'p12', 'p13', 'p14', 'p15', 'p16', 'p17', 'p18', 'p19', 'p20', 'p21', 'p22', 'p23', 'p24', 'p25']\n",
            "['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B10', 'B11']\n"
          ]
        }
      ],
      "source": [
        "# There are 25 strings in the list. \n",
        "RESPONSE = ['p1','p2','p3','p4','p5','p6','p7','p8','p9','p10','p11','p12','p13','p14','p15','p16','p17','p18','p19','p20','p21','p22','p23','p24','p25']\n",
        "print(RESPONSE)\n",
        "print(BANDS)\n",
        "\n",
        "\n",
        "# Create a feature stack by concatenating two image collections and converting the result to float.\n",
        "featureStack = ee.Image.cat([\n",
        "  ls_2017.select(BANDS),\n",
        "  wetland_oneHot.select(RESPONSE)\n",
        "]).float()\n",
        "\n",
        "# Define a kernel that will be used for image processing operations. \n",
        "# The kernel is initialized with a list of ones, meaning that each pixel in the kernel will have a weight of one when applied to the image. \n",
        "list = ee.List.repeat(1, KERNEL_SIZE)\n",
        "lists = ee.List.repeat(list, KERNEL_SIZE)\n",
        "kernel = ee.Kernel.fixed(KERNEL_SIZE, KERNEL_SIZE, lists)\n",
        "\n",
        "# Convert the neighborhood values within the kernel to arrays.\n",
        "# Create a new array image where each pixel value is an array representing the neighborhood of the corresponding pixel in the original feature stack. \n",
        "# THe size of the neighborhood is dertermined by the size of the kernel. \n",
        "# 'neighborhoodToArray' is an EE function that converst an image to an array image. \n",
        "arrays = featureStack.neighborhoodToArray(kernel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLV1CArWZWEe",
        "outputId": "f3850dfe-3bc7-4dcd-958d-53ac5e407f34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'B1': FixedLenFeature(shape=[128, 128], dtype=tf.float32, default_value=None), 'B2': FixedLenFeature(shape=[128, 128], dtype=tf.float32, default_value=None), 'B3': FixedLenFeature(shape=[128, 128], dtype=tf.float32, default_value=None), 'B4': FixedLenFeature(shape=[128, 128], dtype=tf.float32, default_value=None), 'B5': FixedLenFeature(shape=[128, 128], dtype=tf.float32, default_value=None), 'B6': FixedLenFeature(shape=[128, 128], dtype=tf.float32, default_value=None), 'B7': FixedLenFeature(shape=[128, 128], dtype=tf.float32, default_value=None), 'B10': FixedLenFeature(shape=[128, 128], dtype=tf.float32, default_value=None), 'B11': FixedLenFeature(shape=[128, 128], dtype=tf.float32, default_value=None), 'p1': FixedLenFeature(shape=[128, 128], dtype=tf.float32, default_value=None), 'p2': FixedLenFeature(shape=[128, 128], dtype=tf.float32, default_value=None), 'p3': FixedLenFeature(shape=[128, 128], dtype=tf.float32, default_value=None), 'p4': FixedLenFeature(shape=[128, 128], dtype=tf.float32, default_value=None), 'p5': FixedLenFeature(shape=[128, 128], dtype=tf.float32, default_value=None), 'p6': FixedLenFeature(shape=[128, 128], dtype=tf.float32, default_value=None), 'p7': FixedLenFeature(shape=[128, 128], dtype=tf.float32, default_value=None), 'p8': FixedLenFeature(shape=[128, 128], dtype=tf.float32, default_value=None), 'p9': FixedLenFeature(shape=[128, 128], dtype=tf.float32, default_value=None), 'p10': FixedLenFeature(shape=[128, 128], dtype=tf.float32, default_value=None), 'p11': FixedLenFeature(shape=[128, 128], dtype=tf.float32, default_value=None), 'p12': FixedLenFeature(shape=[128, 128], dtype=tf.float32, default_value=None), 'p13': FixedLenFeature(shape=[128, 128], dtype=tf.float32, default_value=None), 'p14': FixedLenFeature(shape=[128, 128], dtype=tf.float32, default_value=None), 'p15': FixedLenFeature(shape=[128, 128], dtype=tf.float32, default_value=None), 'p16': FixedLenFeature(shape=[128, 128], dtype=tf.float32, default_value=None), 'p17': FixedLenFeature(shape=[128, 128], dtype=tf.float32, default_value=None), 'p18': FixedLenFeature(shape=[128, 128], dtype=tf.float32, default_value=None), 'p19': FixedLenFeature(shape=[128, 128], dtype=tf.float32, default_value=None), 'p20': FixedLenFeature(shape=[128, 128], dtype=tf.float32, default_value=None), 'p21': FixedLenFeature(shape=[128, 128], dtype=tf.float32, default_value=None), 'p22': FixedLenFeature(shape=[128, 128], dtype=tf.float32, default_value=None), 'p23': FixedLenFeature(shape=[128, 128], dtype=tf.float32, default_value=None), 'p24': FixedLenFeature(shape=[128, 128], dtype=tf.float32, default_value=None), 'p25': FixedLenFeature(shape=[128, 128], dtype=tf.float32, default_value=None)}\n"
          ]
        }
      ],
      "source": [
        "# RESPONSE = 'b1'\n",
        "\n",
        "FEATURES = BANDS + RESPONSE\n",
        "\n",
        "\n",
        "# Specify the size and shape of patches expected by the model.\n",
        "# KERNEL_SIZE = KERNEL_SIZE\n",
        "KERNEL_SHAPE = [KERNEL_SIZE, KERNEL_SIZE]\n",
        "COLUMNS = [\n",
        "  tf.io.FixedLenFeature(shape=KERNEL_SHAPE, dtype=tf.float32) for k in FEATURES\n",
        "]\n",
        "FEATURES_DICT = dict(zip(FEATURES, COLUMNS))\n",
        "\n",
        "print(FEATURES_DICT)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fD5sj0X-aIXP",
        "outputId": "4d825317-c328-4c49-af25-5415a03c5991"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B10', 'B11', 'p1', 'p2', 'p3', 'p4', 'p5', 'p6', 'p7', 'p8', 'p9', 'p10', 'p11', 'p12', 'p13', 'p14', 'p15', 'p16', 'p17', 'p18', 'p19', 'p20', 'p21', 'p22', 'p23', 'p24', 'p25']\n"
          ]
        }
      ],
      "source": [
        "print(featureStack.bandNames().getInfo())\n",
        "# print(arrays)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zYcnA30aPEj"
      },
      "source": [
        "Use some pre-made geometries to sample the stack in strategic locations. \n",
        "\n",
        "Specifically, these are hand-made polygons in which to take the 128x128 samples. \n",
        "\n",
        "Display the sampling polygons on a map, red for training polygons, blue for evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 796
        },
        "id": "dM41nHujaMvY",
        "outputId": "fc55fa45-aaa9-4af2-9a25-6c58fa9b874e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<folium.folium.Map at 0x7f64ed7972e0>"
            ],
            "text/html": [
              "<div style=\"width:100%;\"><div style=\"position:relative;width:100%;height:0;padding-bottom:60%;\"><span style=\"color:#565656\">Make this Notebook Trusted to load map: File -> Trust Notebook</span><iframe srcdoc=\"&lt;!DOCTYPE html&gt;\n",
              "&lt;html&gt;\n",
              "&lt;head&gt;\n",
              "    \n",
              "    &lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html; charset=UTF-8&quot; /&gt;\n",
              "    \n",
              "        &lt;script&gt;\n",
              "            L_NO_TOUCH = false;\n",
              "            L_DISABLE_3D = false;\n",
              "        &lt;/script&gt;\n",
              "    \n",
              "    &lt;style&gt;html, body {width: 100%;height: 100%;margin: 0;padding: 0;}&lt;/style&gt;\n",
              "    &lt;style&gt;#map {position:absolute;top:0;bottom:0;right:0;left:0;}&lt;/style&gt;\n",
              "    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.js&quot;&gt;&lt;/script&gt;\n",
              "    &lt;script src=&quot;https://code.jquery.com/jquery-1.12.4.min.js&quot;&gt;&lt;/script&gt;\n",
              "    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/js/bootstrap.bundle.min.js&quot;&gt;&lt;/script&gt;\n",
              "    &lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.js&quot;&gt;&lt;/script&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/css/bootstrap.min.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://netdna.bootstrapcdn.com/bootstrap/3.0.0/css/bootstrap.min.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.2.0/css/all.min.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/gh/python-visualization/folium/folium/templates/leaflet.awesome.rotate.min.css&quot;/&gt;\n",
              "    \n",
              "            &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width,\n",
              "                initial-scale=1.0, maximum-scale=1.0, user-scalable=no&quot; /&gt;\n",
              "            &lt;style&gt;\n",
              "                #map_f92ee505f1272306bba6a5a069029169 {\n",
              "                    position: relative;\n",
              "                    width: 100.0%;\n",
              "                    height: 100.0%;\n",
              "                    left: 0.0%;\n",
              "                    top: 0.0%;\n",
              "                }\n",
              "                .leaflet-container { font-size: 1rem; }\n",
              "            &lt;/style&gt;\n",
              "        \n",
              "&lt;/head&gt;\n",
              "&lt;body&gt;\n",
              "    \n",
              "    \n",
              "            &lt;div class=&quot;folium-map&quot; id=&quot;map_f92ee505f1272306bba6a5a069029169&quot; &gt;&lt;/div&gt;\n",
              "        \n",
              "&lt;/body&gt;\n",
              "&lt;script&gt;\n",
              "    \n",
              "    \n",
              "            var map_f92ee505f1272306bba6a5a069029169 = L.map(\n",
              "                &quot;map_f92ee505f1272306bba6a5a069029169&quot;,\n",
              "                {\n",
              "                    center: [21.4, -158.0],\n",
              "                    crs: L.CRS.EPSG3857,\n",
              "                    zoom: 4,\n",
              "                    zoomControl: true,\n",
              "                    preferCanvas: false,\n",
              "                }\n",
              "            );\n",
              "\n",
              "            \n",
              "\n",
              "        \n",
              "    \n",
              "            var tile_layer_40f60e2ecb4a4595fd63a1c8665e0112 = L.tileLayer(\n",
              "                &quot;https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png&quot;,\n",
              "                {&quot;attribution&quot;: &quot;Data by \\u0026copy; \\u003ca target=\\&quot;_blank\\&quot; href=\\&quot;http://openstreetmap.org\\&quot;\\u003eOpenStreetMap\\u003c/a\\u003e, under \\u003ca target=\\&quot;_blank\\&quot; href=\\&quot;http://www.openstreetmap.org/copyright\\&quot;\\u003eODbL\\u003c/a\\u003e.&quot;, &quot;detectRetina&quot;: false, &quot;maxNativeZoom&quot;: 18, &quot;maxZoom&quot;: 18, &quot;minZoom&quot;: 0, &quot;noWrap&quot;: false, &quot;opacity&quot;: 1, &quot;subdomains&quot;: &quot;abc&quot;, &quot;tms&quot;: false}\n",
              "            ).addTo(map_f92ee505f1272306bba6a5a069029169);\n",
              "        \n",
              "    \n",
              "            var tile_layer_9b08085f44fec001173f6e5bf5f86563 = L.tileLayer(\n",
              "                &quot;https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/maps/a03eede290b9576f1540aa598750cd7c-dc8df8525df240fdacb580a88f1acccd/tiles/{z}/{x}/{y}&quot;,\n",
              "                {&quot;attribution&quot;: &quot;Map Data \\u0026copy; \\u003ca href=\\&quot;https://earthengine.google.com/\\&quot;\\u003eGoogle Earth Engine\\u003c/a\\u003e&quot;, &quot;detectRetina&quot;: false, &quot;maxNativeZoom&quot;: 18, &quot;maxZoom&quot;: 18, &quot;minZoom&quot;: 0, &quot;noWrap&quot;: false, &quot;opacity&quot;: 1, &quot;subdomains&quot;: &quot;abc&quot;, &quot;tms&quot;: false}\n",
              "            ).addTo(map_f92ee505f1272306bba6a5a069029169);\n",
              "        \n",
              "    \n",
              "            var layer_control_3c0995c191c0362690e9362621963712 = {\n",
              "                base_layers : {\n",
              "                    &quot;openstreetmap&quot; : tile_layer_40f60e2ecb4a4595fd63a1c8665e0112,\n",
              "                },\n",
              "                overlays :  {\n",
              "                    &quot;training polygons&quot; : tile_layer_9b08085f44fec001173f6e5bf5f86563,\n",
              "                },\n",
              "            };\n",
              "            L.control.layers(\n",
              "                layer_control_3c0995c191c0362690e9362621963712.base_layers,\n",
              "                layer_control_3c0995c191c0362690e9362621963712.overlays,\n",
              "                {&quot;autoZIndex&quot;: true, &quot;collapsed&quot;: true, &quot;position&quot;: &quot;topright&quot;}\n",
              "            ).addTo(map_f92ee505f1272306bba6a5a069029169);\n",
              "        \n",
              "&lt;/script&gt;\n",
              "&lt;/html&gt;\" style=\"position:absolute;width:100%;height:100%;left:0;top:0;border:none !important;\" allowfullscreen webkitallowfullscreen mozallowfullscreen></iframe></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "# FeatureCollection is a colelction of featuers, where each feature is represented as a dictionary of properties and a geometry that describes its spatial extent. \n",
        "# The trainingPolys features in this are used to train the model to recognize land cover types based on the spectral and spatial features extracted from satellite imagery. \n",
        "# The evalPolys features are used to evaluate the accuracy of the trained model. \n",
        "\n",
        "# trainingPolys = ee.FeatureCollection('projects/ee-seismosmsr-landcover/assets/trainingPolys_ccap')\n",
        "# evalPolys = ee.FeatureCollection('projects/ee-seismosmsr-landcover/assets/evalPolys')\n",
        "\n",
        "trainingPolys = ee.FeatureCollection('projects/ee-seismosmsr-landcover/assets/hawaii_not_oahu') \n",
        "evalPolys = ee.FeatureCollection('projects/ee-seismosmsr-landcover/assets/hawaii_oahu')\n",
        "\n",
        "# Create an empty image.\n",
        "# Create a labled image where the pixles belonging to the training set have a value of 1 and evaluation set have a value of 2.\n",
        "polyImage = ee.Image(0).byte().paint(trainingPolys, 1).paint(evalPolys, 2)\n",
        "polyImage = polyImage.updateMask(polyImage)\n",
        "\n",
        "\n",
        "# Use folium to visualize EE image. \n",
        "\n",
        "mapid = polyImage.getMapId({'min': 1, 'max': 2, 'palette': ['red', 'blue']})\n",
        "map = folium.Map(location=[21.4, -158.], zoom_start=4)\n",
        "folium.TileLayer(\n",
        "    tiles=mapid['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='training polygons',\n",
        "  ).add_to(map)\n",
        "map.add_child(folium.LayerControl())\n",
        "map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BpQSamAvhZMx",
        "outputId": "9e4f7ee1-084a-4e5c-efcc-8b71ce4d00c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B10', 'B11', 'p1', 'p2', 'p3', 'p4', 'p5', 'p6', 'p7', 'p8', 'p9', 'p10', 'p11', 'p12', 'p13', 'p14', 'p15', 'p16', 'p17', 'p18', 'p19', 'p20', 'p21', 'p22', 'p23', 'p24', 'p25']\n"
          ]
        }
      ],
      "source": [
        "# Convert the feature collections to lists for iteration.\n",
        "trainingPolysList = trainingPolys.toList(trainingPolys.size())\n",
        "evalPolysList = evalPolys.toList(evalPolys.size())\n",
        "\n",
        "# These numbers determined experimentally.\n",
        "n = 5 # Number of shards in each polygon.\n",
        "N = 200 # Total sample size in each polygon.\n",
        "\n",
        "# Export all the training data (in many pieces), with one task \n",
        "# per geometry.\n",
        "print(BANDS+RESPONSE)\n",
        "# print()\n",
        "for g in range(trainingPolys.size().getInfo()):\n",
        "  geomSample = ee.FeatureCollection([])\n",
        "  for i in range(n):\n",
        "    sample = arrays.sample(\n",
        "      region = ee.Feature(trainingPolysList.get(g)).geometry(), \n",
        "      scale = 30,\n",
        "      numPixels = N / n, # Size of the shard.\n",
        "      seed = i,\n",
        "      tileScale = 8\n",
        "    )\n",
        "    geomSample = geomSample.merge(sample)\n",
        "    \n",
        "  desc = TRAINING_BASE + '_g' + str(g)\n",
        "  task = ee.batch.Export.table.toCloudStorage(\n",
        "    collection = geomSample,\n",
        "    description = desc,\n",
        "    bucket = BUCKET,\n",
        "    fileNamePrefix = FOLDER + '/' + desc,\n",
        "    fileFormat = 'TFRecord'\n",
        "    # selectors = BANDS + RESPONSE\n",
        "  )\n",
        "  task.start()\n",
        "\n",
        "# Export all the evaluation data.\n",
        "for g in range(evalPolys.size().getInfo()):\n",
        "  geomSample = ee.FeatureCollection([])\n",
        "  for i in range(n):\n",
        "    sample = arrays.sample(\n",
        "      region = ee.Feature(evalPolysList.get(g)).geometry(), \n",
        "      scale = 30,\n",
        "      numPixels = N / n,\n",
        "      seed = i,\n",
        "      tileScale = 8\n",
        "    )\n",
        "    geomSample = geomSample.merge(sample)\n",
        "\n",
        "  desc = EVAL_BASE + '_g' + str(g)\n",
        "  task = ee.batch.Export.table.toCloudStorage(\n",
        "    collection = geomSample,\n",
        "    description = desc,\n",
        "    bucket = BUCKET,\n",
        "    fileNamePrefix = FOLDER + '/' + desc,\n",
        "    fileFormat = 'TFRecord'\n",
        "    # selectors = BANDS + RESPONSE\n",
        "  )\n",
        "  task.start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "l2WrHB9U5DRT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7390032a-efef-4c2d-f036-ffba75631571"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<function get_dataset at 0x7f64ec022dc0>\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def parse_tfrecord(example_proto):\n",
        "  \"\"\"The parsing function.\n",
        "  Read a serialized example into the structure defined by FEATURES_DICT.\n",
        "  Args:\n",
        "    example_proto: a serialized Example.\n",
        "  Returns:\n",
        "    A dictionary of tensors, keyed by feature name.\n",
        "  \"\"\"\n",
        "  return tf.io.parse_single_example(example_proto, FEATURES_DICT)\n",
        "\n",
        "\n",
        "def to_tuple(inputs):\n",
        "  \"\"\"Function to convert a dictionary of tensors to a tuple of (inputs, outputs).\n",
        "  Turn the tensors returned by parse_tfrecord into a stack in HWC shape.\n",
        "  Args:\n",
        "    inputs: A dictionary of tensors, keyed by feature name.\n",
        "  Returns:\n",
        "    A tuple of (inputs, outputs).\n",
        "  \"\"\"\n",
        "  inputsList = [inputs.get(key) for key in FEATURES]\n",
        "  stacked = tf.stack(inputsList, axis=0)\n",
        "  # Convert from CHW to HWC\n",
        "  stacked = tf.transpose(stacked, [1, 2, 0])\n",
        "  return stacked[:,:,:len(BANDS)], stacked[:,:,len(BANDS):]\n",
        "\n",
        "\n",
        "def get_dataset(pattern):\n",
        "  \"\"\"Function to read, parse and format to tuple a set of input tfrecord files.\n",
        "  Get all the files matching the pattern, parse and convert to tuple.\n",
        "  Args:\n",
        "    pattern: A file pattern to match in a Cloud Storage bucket.\n",
        "  Returns:\n",
        "    A tf.data.Dataset\n",
        "  \"\"\"\n",
        "  glob = tf.io.gfile.glob(pattern)\n",
        "  dataset = tf.data.TFRecordDataset(glob, compression_type='GZIP')\n",
        "  dataset = dataset.map(parse_tfrecord, num_parallel_calls=5)\n",
        "  dataset = dataset.map(to_tuple, num_parallel_calls=5)\n",
        "  return dataset\n",
        "\n",
        "\n",
        "print(get_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "PYE9NP8j5OWN"
      },
      "outputs": [],
      "source": [
        "# Define a function that returns a preprocessed dataset for ML. \n",
        "# Match the training data files in GCS bucket.\n",
        "# Calls 'get_dataset' function to read, parse, and convert the input files to tuples of tensors. \n",
        "# The resulting dataset is shuffled, batched, and repeated (epochs) to prepare for ML training. \n",
        "# Save the resulting dataset to 'training' variable. \n",
        "def get_training_dataset():\n",
        "\t\"\"\"Get the preprocessed training dataset\n",
        "  Returns: \n",
        "    A tf.data.Dataset of training data.\n",
        "  \"\"\"\n",
        "\tglob = 'gs://' + BUCKET + '/' + FOLDER + '/' + TRAINING_BASE + '*'\n",
        "\t# print(glob)\n",
        "\tdataset = get_dataset(glob)\n",
        "\tdataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
        "\treturn dataset\n",
        "\n",
        "training = get_training_dataset()\n",
        "\n",
        "# print(iter(training.take(2)).next())\n",
        "# This appears to be a tensor with shape (16, 128, 128, 9) and data type float32. \n",
        "# The first dimension has a size of 16, which suggests that this tensor contains a batch of 16 samples. \n",
        "# The next two dimensions have a size of 128, which suggests that each sample has an image with a resolution of 128 x 128 pixels. \n",
        "# The last dimension has a size of 9, which suggests that each pixel is represented by a 9-dimensional feature vector. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNNJCqsJ5RBx",
        "outputId": "a80f986f-fa8c-45a4-8654-e5af75c6a096"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gs://remote_sensing_fuckit_bucket/wetland_unet/eval_patches*\n"
          ]
        }
      ],
      "source": [
        "# Return a Tenserflow dataset contaiing the preprocessed evaluation data. \n",
        "# File path to the evaluation data stored in GCS bucket.\n",
        "# Calls 'get_dataset' function to create a TenserFlow dataset containing the evaluation data. \n",
        "# Evaluation dataset is batched with a batchsize of 1, meaning each element of the dataset consists of a single example. \n",
        "# The function returns the resulting evaluation dataset.\n",
        "def get_eval_dataset():\n",
        "\t\"\"\"Get the preprocessed evaluation dataset\n",
        "  Returns: \n",
        "    A tf.data.Dataset of evaluation data.\n",
        "  \"\"\"\n",
        "\tglob = 'gs://' + BUCKET + '/' + FOLDER + '/' + EVAL_BASE + '*'\n",
        "\tprint(glob)\n",
        "\tdataset = get_dataset(glob)\n",
        "\tdataset = dataset.batch(1).repeat()\n",
        "\treturn dataset\n",
        "\n",
        "evaluation = get_eval_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjIctHU75YAB",
        "outputId": "0ee4f567-5a02-4e6a-9a0c-154bcb3760f7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_RepeatDataset element_spec=(TensorSpec(shape=(None, 128, 128, 9), dtype=tf.float32, name=None), TensorSpec(shape=(None, 128, 128, 25), dtype=tf.float32, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nndrupcf5vFi",
        "outputId": "1e4ea535-1410-41c5-8b7c-4497423defac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.12.0\n"
          ]
        }
      ],
      "source": [
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "HPe--dX05zZX"
      },
      "outputs": [],
      "source": [
        "# import TenserFlow classes and functions \n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import metrics\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "\n",
        "# from tensorflow.python.keras.layers.normalization import BatchNormalization\n",
        "# from tensorflow.python.keras.layers.normalization import BatchNormalization\n",
        "\n",
        "# from tensorflow.keras.layers import BatchNormalization\n",
        "# from keras.layers.normalization.batch_normalization import BatchNormalization\n",
        "# from tensorflow.python.keras.layers import BatchNormalization \n",
        "\n",
        "# U-Net model for image segmentation. \n",
        "# Encoder and decoder conncted by a center block. \n",
        "# Encoder downsamples the input image while capturing its features. \n",
        "# Decoder upsamples the encoded image to generate a segmentation map.\n",
        "def conv_block(input_tensor, num_filters):\n",
        "\tencoder = layers.Conv2D(num_filters, (3, 3), padding='same')(input_tensor)\n",
        "\tencoder = layers.BatchNormalization()(encoder)\n",
        "\tencoder = layers.Activation('relu')(encoder)\n",
        "\tencoder = layers.Conv2D(num_filters, (3, 3), padding='same')(encoder)\n",
        "\tencoder = layers.BatchNormalization()(encoder)\n",
        "\tencoder = layers.Activation('relu')(encoder)\n",
        "\treturn encoder\n",
        "\n",
        "def encoder_block(input_tensor, num_filters):\n",
        "\tencoder = conv_block(input_tensor, num_filters)\n",
        "\tencoder_pool = layers.MaxPooling2D((2, 2), strides=(2, 2))(encoder)\n",
        "\treturn encoder_pool, encoder\n",
        "\n",
        "def decoder_block(input_tensor, concat_tensor, num_filters):\n",
        "\tdecoder = layers.Conv2DTranspose(num_filters, (2, 2), strides=(2, 2), padding='same')(input_tensor)\n",
        "\tdecoder = layers.concatenate([concat_tensor, decoder], axis=-1)\n",
        "\tdecoder = layers.BatchNormalization()(decoder)\n",
        "\tdecoder = layers.Activation('relu')(decoder)\n",
        "\tdecoder = layers.Conv2D(num_filters, (3, 3), padding='same')(decoder)\n",
        "\tdecoder = layers.BatchNormalization()(decoder)\n",
        "\tdecoder = layers.Activation('relu')(decoder)\n",
        "\tdecoder = layers.Conv2D(num_filters, (3, 3), padding='same')(decoder)\n",
        "\tdecoder = layers.BatchNormalization()(decoder)\n",
        "\tdecoder = layers.Activation('relu')(decoder)\n",
        "\treturn decoder\n",
        "\n",
        "def get_model():\n",
        "\tinputs = layers.Input(shape=[KERNEL_SIZE, KERNEL_SIZE, len(BANDS)]) # 256\n",
        "\tencoder0_pool, encoder0 = encoder_block(inputs, 32) # 128\n",
        "\tencoder1_pool, encoder1 = encoder_block(encoder0_pool, 64) # 64\n",
        "\tencoder2_pool, encoder2 = encoder_block(encoder1_pool, 128) # 32\n",
        "\tencoder3_pool, encoder3 = encoder_block(encoder2_pool, 256) # 16\n",
        "\tencoder4_pool, encoder4 = encoder_block(encoder3_pool, 512) # 8\n",
        "\tcenter = conv_block(encoder4_pool, 1024) # center\n",
        "\tdecoder4 = decoder_block(center, encoder4, 512) # 16\n",
        "\tdecoder3 = decoder_block(decoder4, encoder3, 256) # 32\n",
        "\tdecoder2 = decoder_block(decoder3, encoder2, 128) # 64\n",
        "\tdecoder1 = decoder_block(decoder2, encoder1, 64) # 128\n",
        "\tdecoder0 = decoder_block(decoder1, encoder0, 32) # 256\n",
        "\toutputs = layers.Conv2D(25, (1, 1), activation='softmax')(decoder0)\n",
        "\n",
        "\tmodel = models.Model(inputs=[inputs], outputs=[outputs])\n",
        "\n",
        "\tmodel.compile(\n",
        "\t\toptimizer=optimizers.get(OPTIMIZER), \n",
        "\t\tloss=losses.get(LOSS),\n",
        "\t\tmetrics=[metrics.get(metric) for metric in METRICS])\n",
        "\n",
        "\treturn model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXK-0qM42UwP",
        "outputId": "6d7e75c2-514c-4aae-b8b6-edb97e207b31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 128, 128, 9  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv2d_46 (Conv2D)             (None, 128, 128, 32  2624        ['input_3[0][0]']                \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_54 (BatchN  (None, 128, 128, 32  128        ['conv2d_46[0][0]']              \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " activation_54 (Activation)     (None, 128, 128, 32  0           ['batch_normalization_54[0][0]'] \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_47 (Conv2D)             (None, 128, 128, 32  9248        ['activation_54[0][0]']          \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_55 (BatchN  (None, 128, 128, 32  128        ['conv2d_47[0][0]']              \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " activation_55 (Activation)     (None, 128, 128, 32  0           ['batch_normalization_55[0][0]'] \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " max_pooling2d_10 (MaxPooling2D  (None, 64, 64, 32)  0           ['activation_55[0][0]']          \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv2d_48 (Conv2D)             (None, 64, 64, 64)   18496       ['max_pooling2d_10[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_56 (BatchN  (None, 64, 64, 64)  256         ['conv2d_48[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_56 (Activation)     (None, 64, 64, 64)   0           ['batch_normalization_56[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_49 (Conv2D)             (None, 64, 64, 64)   36928       ['activation_56[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_57 (BatchN  (None, 64, 64, 64)  256         ['conv2d_49[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_57 (Activation)     (None, 64, 64, 64)   0           ['batch_normalization_57[0][0]'] \n",
            "                                                                                                  \n",
            " max_pooling2d_11 (MaxPooling2D  (None, 32, 32, 64)  0           ['activation_57[0][0]']          \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv2d_50 (Conv2D)             (None, 32, 32, 128)  73856       ['max_pooling2d_11[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_58 (BatchN  (None, 32, 32, 128)  512        ['conv2d_50[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_58 (Activation)     (None, 32, 32, 128)  0           ['batch_normalization_58[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_51 (Conv2D)             (None, 32, 32, 128)  147584      ['activation_58[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_59 (BatchN  (None, 32, 32, 128)  512        ['conv2d_51[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_59 (Activation)     (None, 32, 32, 128)  0           ['batch_normalization_59[0][0]'] \n",
            "                                                                                                  \n",
            " max_pooling2d_12 (MaxPooling2D  (None, 16, 16, 128)  0          ['activation_59[0][0]']          \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv2d_52 (Conv2D)             (None, 16, 16, 256)  295168      ['max_pooling2d_12[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_60 (BatchN  (None, 16, 16, 256)  1024       ['conv2d_52[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_60 (Activation)     (None, 16, 16, 256)  0           ['batch_normalization_60[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_53 (Conv2D)             (None, 16, 16, 256)  590080      ['activation_60[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_61 (BatchN  (None, 16, 16, 256)  1024       ['conv2d_53[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_61 (Activation)     (None, 16, 16, 256)  0           ['batch_normalization_61[0][0]'] \n",
            "                                                                                                  \n",
            " max_pooling2d_13 (MaxPooling2D  (None, 8, 8, 256)   0           ['activation_61[0][0]']          \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv2d_54 (Conv2D)             (None, 8, 8, 512)    1180160     ['max_pooling2d_13[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_62 (BatchN  (None, 8, 8, 512)   2048        ['conv2d_54[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_62 (Activation)     (None, 8, 8, 512)    0           ['batch_normalization_62[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_55 (Conv2D)             (None, 8, 8, 512)    2359808     ['activation_62[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_63 (BatchN  (None, 8, 8, 512)   2048        ['conv2d_55[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_63 (Activation)     (None, 8, 8, 512)    0           ['batch_normalization_63[0][0]'] \n",
            "                                                                                                  \n",
            " max_pooling2d_14 (MaxPooling2D  (None, 4, 4, 512)   0           ['activation_63[0][0]']          \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv2d_56 (Conv2D)             (None, 4, 4, 1024)   4719616     ['max_pooling2d_14[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_64 (BatchN  (None, 4, 4, 1024)  4096        ['conv2d_56[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_64 (Activation)     (None, 4, 4, 1024)   0           ['batch_normalization_64[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_57 (Conv2D)             (None, 4, 4, 1024)   9438208     ['activation_64[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_65 (BatchN  (None, 4, 4, 1024)  4096        ['conv2d_57[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_65 (Activation)     (None, 4, 4, 1024)   0           ['batch_normalization_65[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_transpose_10 (Conv2DTra  (None, 8, 8, 512)   2097664     ['activation_65[0][0]']          \n",
            " nspose)                                                                                          \n",
            "                                                                                                  \n",
            " concatenate_10 (Concatenate)   (None, 8, 8, 1024)   0           ['activation_63[0][0]',          \n",
            "                                                                  'conv2d_transpose_10[0][0]']    \n",
            "                                                                                                  \n",
            " batch_normalization_66 (BatchN  (None, 8, 8, 1024)  4096        ['concatenate_10[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_66 (Activation)     (None, 8, 8, 1024)   0           ['batch_normalization_66[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_58 (Conv2D)             (None, 8, 8, 512)    4719104     ['activation_66[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_67 (BatchN  (None, 8, 8, 512)   2048        ['conv2d_58[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_67 (Activation)     (None, 8, 8, 512)    0           ['batch_normalization_67[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_59 (Conv2D)             (None, 8, 8, 512)    2359808     ['activation_67[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_68 (BatchN  (None, 8, 8, 512)   2048        ['conv2d_59[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_68 (Activation)     (None, 8, 8, 512)    0           ['batch_normalization_68[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_transpose_11 (Conv2DTra  (None, 16, 16, 256)  524544     ['activation_68[0][0]']          \n",
            " nspose)                                                                                          \n",
            "                                                                                                  \n",
            " concatenate_11 (Concatenate)   (None, 16, 16, 512)  0           ['activation_61[0][0]',          \n",
            "                                                                  'conv2d_transpose_11[0][0]']    \n",
            "                                                                                                  \n",
            " batch_normalization_69 (BatchN  (None, 16, 16, 512)  2048       ['concatenate_11[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_69 (Activation)     (None, 16, 16, 512)  0           ['batch_normalization_69[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_60 (Conv2D)             (None, 16, 16, 256)  1179904     ['activation_69[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_70 (BatchN  (None, 16, 16, 256)  1024       ['conv2d_60[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_70 (Activation)     (None, 16, 16, 256)  0           ['batch_normalization_70[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_61 (Conv2D)             (None, 16, 16, 256)  590080      ['activation_70[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_71 (BatchN  (None, 16, 16, 256)  1024       ['conv2d_61[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_71 (Activation)     (None, 16, 16, 256)  0           ['batch_normalization_71[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_transpose_12 (Conv2DTra  (None, 32, 32, 128)  131200     ['activation_71[0][0]']          \n",
            " nspose)                                                                                          \n",
            "                                                                                                  \n",
            " concatenate_12 (Concatenate)   (None, 32, 32, 256)  0           ['activation_59[0][0]',          \n",
            "                                                                  'conv2d_transpose_12[0][0]']    \n",
            "                                                                                                  \n",
            " batch_normalization_72 (BatchN  (None, 32, 32, 256)  1024       ['concatenate_12[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_72 (Activation)     (None, 32, 32, 256)  0           ['batch_normalization_72[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_62 (Conv2D)             (None, 32, 32, 128)  295040      ['activation_72[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_73 (BatchN  (None, 32, 32, 128)  512        ['conv2d_62[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_73 (Activation)     (None, 32, 32, 128)  0           ['batch_normalization_73[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_63 (Conv2D)             (None, 32, 32, 128)  147584      ['activation_73[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_74 (BatchN  (None, 32, 32, 128)  512        ['conv2d_63[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_74 (Activation)     (None, 32, 32, 128)  0           ['batch_normalization_74[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_transpose_13 (Conv2DTra  (None, 64, 64, 64)  32832       ['activation_74[0][0]']          \n",
            " nspose)                                                                                          \n",
            "                                                                                                  \n",
            " concatenate_13 (Concatenate)   (None, 64, 64, 128)  0           ['activation_57[0][0]',          \n",
            "                                                                  'conv2d_transpose_13[0][0]']    \n",
            "                                                                                                  \n",
            " batch_normalization_75 (BatchN  (None, 64, 64, 128)  512        ['concatenate_13[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_75 (Activation)     (None, 64, 64, 128)  0           ['batch_normalization_75[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_64 (Conv2D)             (None, 64, 64, 64)   73792       ['activation_75[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_76 (BatchN  (None, 64, 64, 64)  256         ['conv2d_64[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_76 (Activation)     (None, 64, 64, 64)   0           ['batch_normalization_76[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_65 (Conv2D)             (None, 64, 64, 64)   36928       ['activation_76[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_77 (BatchN  (None, 64, 64, 64)  256         ['conv2d_65[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_77 (Activation)     (None, 64, 64, 64)   0           ['batch_normalization_77[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_transpose_14 (Conv2DTra  (None, 128, 128, 32  8224       ['activation_77[0][0]']          \n",
            " nspose)                        )                                                                 \n",
            "                                                                                                  \n",
            " concatenate_14 (Concatenate)   (None, 128, 128, 64  0           ['activation_55[0][0]',          \n",
            "                                )                                 'conv2d_transpose_14[0][0]']    \n",
            "                                                                                                  \n",
            " batch_normalization_78 (BatchN  (None, 128, 128, 64  256        ['concatenate_14[0][0]']         \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " activation_78 (Activation)     (None, 128, 128, 64  0           ['batch_normalization_78[0][0]'] \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_66 (Conv2D)             (None, 128, 128, 32  18464       ['activation_78[0][0]']          \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_79 (BatchN  (None, 128, 128, 32  128        ['conv2d_66[0][0]']              \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " activation_79 (Activation)     (None, 128, 128, 32  0           ['batch_normalization_79[0][0]'] \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_67 (Conv2D)             (None, 128, 128, 32  9248        ['activation_79[0][0]']          \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_80 (BatchN  (None, 128, 128, 32  128        ['conv2d_67[0][0]']              \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " activation_80 (Activation)     (None, 128, 128, 32  0           ['batch_normalization_80[0][0]'] \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_68 (Conv2D)             (None, 128, 128, 25  825         ['activation_80[0][0]']          \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 31,129,017\n",
            "Trainable params: 31,113,017\n",
            "Non-trainable params: 16,000\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "m = get_model()\n",
        "print(m.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G39jZYYX6B0E",
        "outputId": "640ea691-0238-4489-ff18-edbca6f2c58f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n"
          ]
        }
      ],
      "source": [
        "m = get_model()\n",
        "tf.keras.utils.plot_model(m)\n",
        "# Load a trained model. 50 epochs. 25 hours. Final RMSE ~0.08.\n",
        "MODEL_DIR = 'gs://' + BUCKET +'/'+ FOLDER +  '/'\n",
        "\n",
        "# m = tf.keras.models.load_model(MODEL_DIR)\n",
        "# EPOCHS\n",
        "#int(TRAIN_SIZE / BATCH_SIZE)\n",
        "m.fit(\n",
        "    x=training, \n",
        "    epochs=300, \n",
        "    steps_per_epoch=73, \n",
        "    validation_data=evaluation,\n",
        "    validation_steps=EVAL_SIZE)\n",
        "\n",
        "m.save('gs://' + BUCKET +'/'+ FOLDER +  '/')\n",
        "print(m)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def doExport(out_image_base, shape, region):\n",
        "  \"\"\"Run the image export task.  Block until complete.\n",
        "  \"\"\"\n",
        "  task = ee.batch.Export.image.toDrive(\n",
        "    image = ls_2017.select(BANDS),\n",
        "    description = out_image_base,\n",
        "    fileNamePrefix = out_image_base,\n",
        "    folder = FOLDER,\n",
        "    region = region.getInfo()['coordinates'],\n",
        "    scale = 30,\n",
        "    fileFormat = 'TFRecord',\n",
        "    maxPixels = 1e10,\n",
        "    formatOptions = {\n",
        "      'patchDimensions': shape,\n",
        "      'compressed': True,\n",
        "      'maxFileSize': 104857600\n",
        "    }\n",
        "  )\n",
        "  task.start()\n",
        "\n",
        "  # Block until the task completes.\n",
        "  print('Running image export to Google Drive...')\n",
        "  import time\n",
        "  while task.active():\n",
        "    time.sleep(30)\n",
        "\n",
        "  # Error condition\n",
        "  if task.status()['state'] != 'COMPLETED':\n",
        "    print('Error with image export.')\n",
        "  else:\n",
        "    print('Image export completed.')"
      ],
      "metadata": {
        "id": "Ld7W6gm80tv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def doPrediction(out_image_base, kernel_shape, region):\n",
        "  \"\"\"Perform inference on exported imagery.\n",
        "  \"\"\"\n",
        "\n",
        "  print('Looking for TFRecord files...')\n",
        "\n",
        "  # Get a list of all the files in the output bucket.\n",
        "  filesList = os.listdir(op.join(ROOT_DIR, FOLDER))\n",
        "\n",
        "  # Get only the files generated by the image export.\n",
        "  exportFilesList = [s for s in filesList if out_image_base in s]\n",
        "\n",
        "  # Get the list of image files and the JSON mixer file.\n",
        "  imageFilesList = []\n",
        "  jsonFile = None\n",
        "  for f in exportFilesList:\n",
        "    if f.endswith('.tfrecord.gz'):\n",
        "      imageFilesList.append(op.join(ROOT_DIR, FOLDER, f))\n",
        "    elif f.endswith('.json'):\n",
        "      jsonFile = f\n",
        "\n",
        "  # Make sure the files are in the right order.\n",
        "  imageFilesList.sort()\n",
        "\n",
        "  from pprint import pprint\n",
        "  pprint(imageFilesList)\n",
        "  print(jsonFile)\n",
        "\n",
        "  import json\n",
        "  # Load the contents of the mixer file to a JSON object.\n",
        "  with open(op.join(ROOT_DIR, FOLDER, jsonFile), 'r') as f:\n",
        "    mixer = json.load(f)\n",
        "\n",
        "  pprint(mixer)\n",
        "  patches = mixer['totalPatches']\n",
        "\n",
        "  # Get set up for prediction.\n",
        "\n",
        "  imageColumns = [\n",
        "    tf.io.FixedLenFeature(shape=kernel_shape, dtype=tf.float32) \n",
        "      for k in BANDS\n",
        "  ]\n",
        "\n",
        "  imageFeaturesDict = dict(zip(BANDS, imageColumns))\n",
        "\n",
        "  def parse_image(example_proto):\n",
        "    return tf.io.parse_single_example(example_proto, imageFeaturesDict)\n",
        "\n",
        "  def toTupleImage(inputs):\n",
        "    inputsList = [inputs.get(key) for key in BANDS]\n",
        "    stacked = tf.stack(inputsList, axis=0)\n",
        "    stacked = tf.transpose(stacked, [1, 2, 0])\n",
        "    return stacked\n",
        "\n",
        "   # Create a dataset from the TFRecord file(s) in Cloud Storage.\n",
        "  imageDataset = tf.data.TFRecordDataset(imageFilesList, compression_type='GZIP')\n",
        "  imageDataset = imageDataset.map(parse_image, num_parallel_calls=5)\n",
        "  imageDataset = imageDataset.map(toTupleImage).batch(1)\n",
        "\n",
        "  # Perform inference.\n",
        "  print('Running predictions...')\n",
        "  predictions = model.predict(imageDataset, steps=patches, verbose=1)\n",
        "  # print(predictions[0])\n",
        "\n",
        "  print('Writing predictions...')\n",
        "  out_image_file = op.join(ROOT_DIR, FOLDER, f'{out_image_base}pred.TFRecord')\n",
        "  writer = tf.io.TFRecordWriter(out_image_file)\n",
        "  patches = 0\n",
        "  for predictionPatch in predictions:\n",
        "    print('Writing patch ' + str(patches) + '...')\n",
        "    predictionPatch = tf.argmax(predictionPatch, axis=2)\n",
        "\n",
        "    # Create an example.\n",
        "    example = tf.train.Example(\n",
        "      features=tf.train.Features(\n",
        "        feature={\n",
        "          'class': tf.train.Feature(\n",
        "              float_list=tf.train.FloatList(\n",
        "                  value=predictionPatch.numpy().flatten()))\n",
        "        }\n",
        "      )\n",
        "    )\n",
        "    # Write the example.\n",
        "    writer.write(example.SerializeToString())\n",
        "    patches += 1\n",
        "\n",
        "  writer.close()"
      ],
      "metadata": {
        "id": "AMwcooPI02OX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Output assets folder: YOUR FOLDER\n",
        "user_folder = 'users/seismosmsr/UNET_regression' # INSERT YOUR FOLDER HERE.\n",
        "\n",
        "# Base file name to use for TFRecord files and assets.\n",
        "image_base = 'FCNN_demo_Oahu'\n",
        "# Half this will extend on the sides of each patch.\n",
        "KERNEL_SHAPE = [128, 128]\n",
        "# Beijing\n",
        "#-123.3152617122404422,37.7933818681806812 : -121.5833326464016722,38.6741999222186763\n",
        "OahuDistricts = ee.FeatureCollection('projects/ee-seismosmsr-landcover/assets/Neighborhood_Board_Subdistricts')\n",
        "query_str = f'SD_DESC == \"Makaha\"'\n",
        "makaha = OahuDistricts.filter(query_str).geometry() \n",
        "# makaha.getInfo()"
      ],
      "metadata": {
        "id": "jK6xOeE-3UYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doExport(image_base, KERNEL_SHAPE,makaha)"
      ],
      "metadata": {
        "id": "domZ7b_13cbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount our Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "2rO3R07C3enC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os as os\n",
        "from os import path as op\n",
        "ROOT_DIR = '/content/drive/My Drive/'\n",
        "doPrediction(image_base, KERNEL_SHAPE, makaha)"
      ],
      "metadata": {
        "id": "ONh2ltTY3kFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out_image = ee.Image('projects/ee-seismosmsr-landcover/assets/FCNN_demo_Oahu-mixer')\n",
        "mapid = out_image.getMapId({'min': 2, 'max': 3, 'palette': ['00A600','63C600','E6E600','E9BD3A','ECB176','EFC2B3','F2F2F2']})\n",
        "map = folium.Map(location=[21.4, -158])\n",
        "# map = folium.Map(location=[              \n",
        "#               -29.177943749121233,\n",
        "#               30.55984497070313,\n",
        "# ])\n",
        "folium.TileLayer(\n",
        "    tiles=mapid['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='predicted crop type',\n",
        "  ).add_to(map)\n",
        "map.add_child(folium.LayerControl())\n",
        "map"
      ],
      "metadata": {
        "id": "A2oItq-T3llR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzM45ZQNvVnH"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}