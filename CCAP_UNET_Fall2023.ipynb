{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aml7hawaiiedu/CCAPLandCoverProject/blob/main/CCAP_UNET_Fall2023.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfCSt4rxG490"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rasterio scikit-image tensorflow keras gdown\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "w8q0OqJ9MdDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import gdown\n",
        "import zipfile\n",
        "import cv2\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import shutil\n",
        "import rasterio\n",
        "import rasterio.plot\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import load_model\n",
        "from keras.utils import Sequence, to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from skimage.transform import resize\n",
        "from skimage.util import random_noise\n",
        "from scipy import ndimage\n",
        "from scipy.ndimage import label as nd_label\n",
        "from scipy.ndimage import generic_filter\n",
        "from scipy.stats import mode\n",
        "\n",
        "# Additional code can be added here if needed"
      ],
      "metadata": {
        "id": "6IvzmnSDMqqQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "directory_path = '/content/image_subsets'\n",
        "os.makedirs(directory_path, exist_ok=True)"
      ],
      "metadata": {
        "id": "u6DickGeN582"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zip_files = glob.glob('/content/drive/MyDrive/wetland_unet/UNET_Image_Chips/imagechip_trainingdata/*.zip')\n",
        "extract_dir = '/content/image_subsets' # destination directory\n",
        "for zip_file in zip_files:\n",
        "    base_name = os.path.basename(zip_file)[:-4]\n",
        "    unzip_dir = os.path.join(extract_dir, base_name)\n",
        "    os.makedirs(unzip_dir, exist_ok=True)\n",
        "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "        zip_ref.extractall(unzip_dir)"
      ],
      "metadata": {
        "id": "CMmXtKr-s7QG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "csv_files = glob.glob('/content/drive/MyDrive/wetland_unet/UNET_Image_Chips/imagechip_trainingdata/*.csv')\n",
        "csv_list = []\n",
        "for csv_file in csv_files:\n",
        "    base_name = os.path.basename(csv_file)[:-4]\n",
        "    csv_dir = os.path.join(extract_dir, base_name)\n",
        "    base_csv = pd.read_csv(csv_file)\n",
        "    base_csv['subset'] = base_name\n",
        "    csv_list.append(base_csv)\n",
        "    index_csv = pd.concat(csv_list, ignore_index=True)"
      ],
      "metadata": {
        "id": "OHIwIVH5uLtH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# csv_files"
      ],
      "metadata": {
        "id": "5yz4whZOvMJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# index_csv"
      ],
      "metadata": {
        "id": "Cuyf0HIBvLnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_rows_df = index_csv.groupby(['tif_name', 'label', 'subset']).size().reset_index(name='Count')\n",
        "unique_rows_df = index_csv.groupby(['tif_name', 'label', 'subset']).agg({'percent': 'mean'}).reset_index()"
      ],
      "metadata": {
        "id": "pYYNUe-22LQr"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# unique_rows_df"
      ],
      "metadata": {
        "id": "8l_TBihNwQf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pivot_df = unique_rows_df.pivot(index=['tif_name', 'subset'], columns='label', values='percent').reset_index().fillna(0)"
      ],
      "metadata": {
        "id": "9-ewXrj8jsS2"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pivot_df"
      ],
      "metadata": {
        "id": "NKPgX0VdxBaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# index_csv"
      ],
      "metadata": {
        "id": "u2r_mfu3xXkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df = index_csv.merge(pivot_df, on=['tif_name', 'subset'], how='left')\n",
        "merged_df.drop(columns=['label'], inplace=True)"
      ],
      "metadata": {
        "id": "s6rnI3Quwdc2"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# merged_df"
      ],
      "metadata": {
        "id": "0Qd5C-G3wgdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sum_df = merged_df.drop_duplicates(subset=['tif_name', 'subset'])"
      ],
      "metadata": {
        "id": "hG9kwGPlxrKn"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sum_df"
      ],
      "metadata": {
        "id": "m2dIvijAyXo-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_rows = len(sum_df)\n",
        "train_fraction = 0.9 # modify this to set the training percentage\n",
        "train_rows = int(total_rows * train_fraction)\n",
        "val_rows = total_rows - train_rows\n",
        "\n",
        "random_assignment = np.array([0] * train_rows + [1] * val_rows)\n",
        "np.random.seed(42)\n",
        "np.random.shuffle(random_assignment)\n",
        "sum_df['random_split'] = random_assignment"
      ],
      "metadata": {
        "id": "WSQZb3slylUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agg_df = sum_df.groupby(['random_split']).agg({13: 'mean',14:'mean',15: 'mean',16:'mean',17: 'mean',18:'mean'}).reset_index()\n",
        "print(agg_df)"
      ],
      "metadata": {
        "id": "BhdSkVK-1UI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sum_df"
      ],
      "metadata": {
        "id": "Mo7NAKJwCUyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sum_df['Images_path']=\"/content/image_subsets/\"+sum_df['subset']+'/Images/'+sum_df['tif_name']\n",
        "sum_df['Labels_path']=\"/content/image_subsets/\"+sum_df['subset']+'/Labels/'+sum_df['tif_name']"
      ],
      "metadata": {
        "id": "CT5lBLaAFtSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading data from geotiff files\n",
        "def load_data(files):\n",
        "    data = []\n",
        "    for file in files:\n",
        "        with rasterio.open(file) as src:\n",
        "            band_data = []\n",
        "            for band in src.read():\n",
        "                band_data.append(band)\n",
        "            data.append(np.dstack(band_data))\n",
        "    return np.array(data)"
      ],
      "metadata": {
        "id": "ykbrnRUW_J1F"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load_data([i for i in  sum_df['Images_path'][0:5]])"
      ],
      "metadata": {
        "id": "bl1tswrFBMX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # example to show the width, height and bands of the images\n",
        "# def get_image_shapes_in_folders(folder_paths):\n",
        "#     image_shapes = []\n",
        "#     for folder_path in folder_paths:\n",
        "#         for root, dirs, files in os.walk(folder_path):\n",
        "#             # Sort the files alphabetically\n",
        "#             files = sorted(files)\n",
        "#             for file in files:\n",
        "#                 if file.endswith('.tif') or file.endswith('.jpg') or file.endswith('.png'):\n",
        "#                     image_path = os.path.join(root, file)\n",
        "#                     try:\n",
        "#                         with rasterio.open(image_path) as src:\n",
        "#                             width, height = src.width, src.height\n",
        "#                             band_count = src.count  # Number of bands in the image\n",
        "#                             image_shapes.append((file, width, height, band_count))\n",
        "#                     except Exception as e:\n",
        "#                         print(f\"Error getting shape of image '{file}': {e}\")\n",
        "#     return image_shapes\n",
        "\n",
        "# folder_paths = [\"/content/image_subsets/Hawaii_2005_005_subset/Images\"]\n",
        "\n",
        "# shapes = get_image_shapes_in_folders(folder_paths)\n",
        "# for shape in shapes:\n",
        "#     file, width, height, band_count = shape\n",
        "#     print(f\" {file[:-4]}: {width}, {height}, {band_count}\")"
      ],
      "metadata": {
        "id": "4UqK2FHq40qU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_reshape_image(image_path, img_height, img_width):\n",
        "    with rasterio.open(image_path) as src:\n",
        "        image = src.read()\n",
        "        return image"
      ],
      "metadata": {
        "id": "QUS8UV5x_NAi"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [load_and_reshape_image(i,512,512) for i in sum_df['Labels_path'][0:5]]"
      ],
      "metadata": {
        "id": "Mpg8plmIJnIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_images_and_labels(image_files, label_files, img_height, img_width, num_classes):\n",
        "  images = []\n",
        "  labels = []\n",
        "\n",
        "  # image_files = glob.glob(os.path.join(image_files, \"*.tif\"))\n",
        "  for image_file in image_files:\n",
        "      image = load_and_reshape_image(image_file, img_height, img_width)\n",
        "      images.append(image)\n",
        "\n",
        "  # label_files = glob.glob(os.path.join(label_files, \"*.tif\"))\n",
        "  for label_file in label_files:\n",
        "      label = load_and_reshape_image(label_file, img_height, img_width)\n",
        "      label -= 1  # adjust labels to be in the range 0-8 instead of 1-9\n",
        "      label = to_categorical(label, num_classes=num_classes)   # one-hot encode the labels\n",
        "      labels.append(label)\n",
        "\n",
        "  return np.array(images), np.array(labels)"
      ],
      "metadata": {
        "id": "ZCase72Y_k6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load_images_and_labels(sum_df['Images_path'][0:5], sum_df['Labels_path'][0:5],512,512,21)"
      ],
      "metadata": {
        "id": "A1nRLa1WKMZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataGenerator(Sequence):\n",
        "    def __init__(self, image_files, label_files, img_height, img_width, batch_size, num_classes):\n",
        "        self.image_files = image_files\n",
        "        self.label_files = label_files\n",
        "        self.img_height = img_height\n",
        "        self.img_width = img_width\n",
        "        self.batch_size = batch_size\n",
        "        self.num_classes = num_classes\n",
        "        # self.noise = noise\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.image_files) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        batch_files = self.image_files[index * self.batch_size : (index + 1) * self.batch_size]\n",
        "        batch_images, batch_labels = self.load_images_and_labels(batch_files)\n",
        "        return batch_images, batch_labels\n",
        "\n",
        "    def load_and_reshape_image(self, image_path):\n",
        "        with rasterio.open(image_path) as src:\n",
        "            image = src.read()\n",
        "            image = image.transpose((1, 2, 0))\n",
        "            if image.shape[0] != self.img_height or image.shape[1] != self.img_width:\n",
        "                image = cv2.resize(image, (self.img_width, self.img_height), interpolation=cv2.INTER_NEAREST)\n",
        "            if len(image.shape) == 3 and image.shape[2] == 1:\n",
        "                image = np.squeeze(image, axis=2)\n",
        "\n",
        "            return image\n",
        "\n",
        "    def load_images_and_labels(self, image_files):\n",
        "        images = []\n",
        "        labels = []\n",
        "\n",
        "        for image_file in image_files:\n",
        "            image = self.load_and_reshape_image(image_file)\n",
        "            image[image <= -3e+38] = np.nan\n",
        "\n",
        "            # # Replace NaN values with the mean of the non-NaN pixels\n",
        "            if np.any(np.isnan(image)):\n",
        "                nan_mask = np.isnan(image)\n",
        "                image[nan_mask] = np.nanmean(image)\n",
        "\n",
        "            # # Replace Inf values with the mean of the non-Inf pixels\n",
        "            if np.any(np.isinf(image)):\n",
        "                inf_mask = np.isinf(image)\n",
        "                image[inf_mask] = np.nanmean(image)\n",
        "\n",
        "            # Convert to float\n",
        "            image = image.astype(np.float32)\n",
        "\n",
        "            # # Z-score normalization\n",
        "            # mean = np.mean(image, axis=(0, 1), keepdims=True)\n",
        "            # std = np.std(image, axis=(0, 1), keepdims=True)\n",
        "            # # mean[mean < 0]\n",
        "            # std[std < 0] = 0\n",
        "\n",
        "            # # Normalize with epsilon to prevent divide by zero\n",
        "            # epsilon = 1e-7\n",
        "\n",
        "            # image = (image - mean) / (std + epsilon)\n",
        "\n",
        "            images.append(image)\n",
        "\n",
        "        for image_file in image_files:\n",
        "            label_file = image_file.replace(\"Images\", \"Labels\")\n",
        "            label = self.load_and_reshape_image(label_file)\n",
        "            label -= 1\n",
        "            label = to_categorical(label, num_classes=self.num_classes)\n",
        "            labels.append(label)\n",
        "\n",
        "        return np.array(images), np.array(labels)"
      ],
      "metadata": {
        "id": "rEDVI98o_0np"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import TenserFlow classes and functions\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import metrics\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "\n",
        "# from tensorflow.python.keras.layers.normalization import BatchNormalization\n",
        "# from tensorflow.python.keras.layers.normalization import BatchNormalization\n",
        "\n",
        "# from tensorflow.keras.layers import BatchNormalization\n",
        "# from keras.layers.normalization.batch_normalization import BatchNormalization\n",
        "# from tensorflow.python.keras.layers import BatchNormalization\n",
        "\n",
        "# U-Net model for image segmentation.\n",
        "# Encoder and decoder conncted by a center block.\n",
        "# Encoder downsamples the input image while capturing its features.\n",
        "# Decoder upsamples the encoded image to generate a segmentation map.\n",
        "def conv_block(input_tensor, num_filters):\n",
        "\tencoder = layers.Conv2D(num_filters, (3, 3), padding='same')(input_tensor)\n",
        "\tencoder = layers.BatchNormalization()(encoder)\n",
        "\tencoder = layers.Activation('relu')(encoder)\n",
        "\tencoder = layers.Conv2D(num_filters, (3, 3), padding='same')(encoder)\n",
        "\tencoder = layers.BatchNormalization()(encoder)\n",
        "\tencoder = layers.Activation('relu')(encoder)\n",
        "\treturn encoder\n",
        "\n",
        "def encoder_block(input_tensor, num_filters):\n",
        "\tencoder = conv_block(input_tensor, num_filters)\n",
        "\tencoder_pool = layers.MaxPooling2D((2, 2), strides=(2, 2))(encoder)\n",
        "\treturn encoder_pool, encoder\n",
        "\n",
        "def decoder_block(input_tensor, concat_tensor, num_filters):\n",
        "\tdecoder = layers.Conv2DTranspose(num_filters, (2, 2), strides=(2, 2), padding='same')(input_tensor)\n",
        "\tdecoder = layers.concatenate([concat_tensor, decoder], axis=-1)\n",
        "\tdecoder = layers.BatchNormalization()(decoder)\n",
        "\tdecoder = layers.Activation('relu')(decoder)\n",
        "\tdecoder = layers.Conv2D(num_filters, (3, 3), padding='same')(decoder)\n",
        "\tdecoder = layers.BatchNormalization()(decoder)\n",
        "\tdecoder = layers.Activation('relu')(decoder)\n",
        "\tdecoder = layers.Conv2D(num_filters, (3, 3), padding='same')(decoder)\n",
        "\tdecoder = layers.BatchNormalization()(decoder)\n",
        "\tdecoder = layers.Activation('relu')(decoder)\n",
        "\treturn decoder\n",
        "\n",
        "def get_model():\n",
        "\tinputs = layers.Input(shape=[KERNEL_SIZE, KERNEL_SIZE, len(BANDS)]) # 256\n",
        "\tencoder0_pool, encoder0 = encoder_block(inputs, 32) # 128\n",
        "\tencoder1_pool, encoder1 = encoder_block(encoder0_pool, 64) # 64\n",
        "\tencoder2_pool, encoder2 = encoder_block(encoder1_pool, 128) # 32\n",
        "\tencoder3_pool, encoder3 = encoder_block(encoder2_pool, 256) # 16\n",
        "\tencoder4_pool, encoder4 = encoder_block(encoder3_pool, 512) # 8\n",
        "\tcenter = conv_block(encoder4_pool, 1024) # center\n",
        "\tdecoder4 = decoder_block(center, encoder4, 512) # 16\n",
        "\tdecoder3 = decoder_block(decoder4, encoder3, 256) # 32\n",
        "\tdecoder2 = decoder_block(decoder3, encoder2, 128) # 64\n",
        "\tdecoder1 = decoder_block(decoder2, encoder1, 64) # 128\n",
        "\tdecoder0 = decoder_block(decoder1, encoder0, 32) # 256\n",
        "\toutputs = layers.Conv2D(25, (1, 1), activation='softmax')(decoder0)\n",
        "\n",
        "\tmodel = models.Model(inputs=[inputs], outputs=[outputs])\n",
        "\n",
        "\tmodel.compile(\n",
        "\t\toptimizer=optimizers.get(OPTIMIZER),\n",
        "\t\tloss=losses.get(LOSS),\n",
        "\t\tmetrics=[metrics.get(metric) for metric in METRICS])\n",
        "\n",
        "\treturn model"
      ],
      "metadata": {
        "id": "R9gKYuRQMUM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(img_size, num_classes):\n",
        "    inputs = keras.Input(shape=(img_size[0], img_size[1], 4))  # Change the number of channels to 4\n",
        "\n",
        "    # Entry block\n",
        "    x = layers.Conv2D(32, 3, strides=2, padding=\"same\")(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation(\"relu\")(x)\n",
        "\n",
        "    previous_block_activation = x  # Set aside residual\n",
        "\n",
        "    # Blocks 1, 2, 3 are identical apart from the feature depth.\n",
        "    for filters in [64, 128, 256]:\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
        "\n",
        "        # Project residual\n",
        "        residual = layers.Conv2D(filters, 1, strides=2, padding=\"same\")(\n",
        "            previous_block_activation\n",
        "        )\n",
        "        x = layers.add([x, residual])  # Add back residual\n",
        "        previous_block_activation = x  # Set aside next residual\n",
        "\n",
        "    for filters in [256, 128, 64, 32]:\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "        x = layers.UpSampling2D(2)(x)\n",
        "\n",
        "        residual = layers.UpSampling2D(2)(previous_block_activation)\n",
        "        residual = layers.Conv2D(filters, 1, padding=\"same\")(residual)\n",
        "        x = layers.add([x, residual])  # Add back residual\n",
        "        previous_block_activation = x  # Set aside next residual\n",
        "\n",
        "    # Add a per-pixel classification layer\n",
        "    outputs = layers.Conv2D(num_classes, 3, activation=\"softmax\", padding=\"same\")(x)\n",
        "\n",
        "    # Define the model\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    model.compile(optimizer=\"adam\", loss='sparse_categorical_crossentropy', metrics=['accuracy'])  # updated to sparse categorical cross-entropy loss\n",
        "    return model"
      ],
      "metadata": {
        "id": "06iTebsEACQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_and_save_segments(input_folder, output_folder, model, img_height, img_width):\n",
        "      # Create output folder if it doesn't exist\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    # Get a list of input files\n",
        "    input_files = [f for f in os.listdir(input_folder) if f.endswith('.tif')]\n",
        "\n",
        "    for filename in input_files:\n",
        "              # Read input image\n",
        "        image_path = os.path.join(input_folder, filename)\n",
        "\n",
        "        with rasterio.open(image_path) as src:\n",
        "            # Read image data and reshape\n",
        "            image = src.read()  # Read all bands\n",
        "\n",
        "        image = load_and_reshape_image(image_path,img_height, img_width)\n",
        "        image = image.astype(np.uint8)\n",
        "        # print(image.shape)\n",
        "        masks = mask_generator.generate(image)\n",
        "\n",
        "        flat_mask = show_anns(masks)\n",
        "        imagery_file = rasterio.open(image_path)\n",
        "        imagery_transform = imagery_file.transform\n",
        "        reshaped_image = rasterio.plot.reshape_as_raster(flat_mask)\n",
        "        reshaped_image = reshaped_image[0]\n",
        "        # Get metadata from the input image\n",
        "        # print(reshaped_image.shape)\n",
        "        meta = src.meta\n",
        "\n",
        "        # Update metadata for the output image\n",
        "        meta.update(count=1, dtype=reshaped_image.dtype)\n",
        "\n",
        "        # Create output path\n",
        "        output_path = os.path.join(output_folder, filename)\n",
        "\n",
        "        # Write all 9 prediction channels as separate bands\n",
        "        with rasterio.open(output_path, 'w', **meta) as dst:\n",
        "            # for i in range(9):\n",
        "            dst.write(reshaped_image,1)  # Write each channel as a separate band\n",
        "\n",
        "        print(f\"Saved prediction for {filename}\")\n",
        "\n",
        "    print(\"Prediction and saving completed.\")"
      ],
      "metadata": {
        "id": "k685DpwEAEzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_and_save(input_folder, output_folder, model, img_height, img_width):\n",
        "    # Create output folder if it doesn't exist\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    # Get a list of input files\n",
        "    input_files = [f for f in os.listdir(input_folder) if f.endswith('.tif')]\n",
        "\n",
        "    for filename in input_files:\n",
        "        # Read input image\n",
        "        input_path = os.path.join(input_folder, filename)\n",
        "        with rasterio.open(input_path) as src:\n",
        "            # Read image data and reshape\n",
        "            image = src.read()  # Read all bands\n",
        "\n",
        "            image[image <= -3e+38] = np.nan\n",
        "\n",
        "            # # Replace NaN values with the mean of the non-NaN pixels\n",
        "            if np.any(np.isnan(image)):\n",
        "                nan_mask = np.isnan(image)\n",
        "                image[nan_mask] = np.nanmean(image)\n",
        "\n",
        "            # # Replace Inf values with the mean of the non-Inf pixels\n",
        "            if np.any(np.isinf(image)):\n",
        "                inf_mask = np.isinf(image)\n",
        "                image[inf_mask] = np.nanmean(image)\n",
        "\n",
        "            # Convert to float\n",
        "            image = image.astype(np.float32)\n",
        "\n",
        "            # # Z-score normalization\n",
        "            # mean = np.mean(image, axis=(0, 1), keepdims=True)\n",
        "            # std = np.std(image, axis=(0, 1), keepdims=True)\n",
        "            # # mean[mean < 0]\n",
        "            # std[std < 0] = 0\n",
        "\n",
        "            # # Normalize with epsilon to prevent divide by zero\n",
        "            # epsilon = 1e-7\n",
        "\n",
        "            # image = (image - mean) / (std + epsilon)\n",
        "\n",
        "            image = np.transpose(image, (1, 2, 0))  # Transpose to (height, width, bands)\n",
        "            image = cv2.resize(image, (img_width, img_height), interpolation=cv2.INTER_NEAREST)\n",
        "            image = np.expand_dims(image, axis=0)  # Add batch dimension\n",
        "            # # Perform prediction\n",
        "            prediction = model.predict(image)\n",
        "            prediction[prediction <= 0] = np.nan\n",
        "            prediction = prediction*255\n",
        "            prediction = prediction.astype(np.uint8)\n",
        "\n",
        "        # Get metadata from the input image\n",
        "        meta = src.meta\n",
        "\n",
        "        # Update metadata for the output image\n",
        "        meta.update(count=26, dtype=prediction.dtype,nodata = 0)\n",
        "        # meta.\n",
        "        # Create output path\n",
        "        output_path = os.path.join(output_folder, filename)\n",
        "\n",
        "        # Write all 9 prediction channels as separate bands\n",
        "        with rasterio.open(output_path, 'w', **meta) as dst:\n",
        "            for i in range(25):\n",
        "                dst.write(prediction[0, :, :, i], i + 1)  # Write each channel as a separate band\n",
        "\n",
        "            # Add a 10th band containing the argmax of the 9 channels\n",
        "            argmax_band = np.argmax(prediction[0], axis=-1)\n",
        "            dst.write(argmax_band, 26)\n",
        "\n",
        "        print(f\"Saved prediction for {filename}\")\n",
        "\n",
        "    print(\"Prediction and saving completed.\")"
      ],
      "metadata": {
        "id": "6Z-mnFB7AL-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "KERNEL_SIZE = 512\n",
        "BANDS = range(7)\n",
        "KERNEL_SHAPE = [KERNEL_SIZE, KERNEL_SIZE]\n",
        "OPTIMIZER = 'adam'\n",
        "LOSS = 'categorical_crossentropy'\n",
        "METRICS = ['categorical_accuracy']\n",
        "\n"
      ],
      "metadata": {
        "id": "JFpe-4LAd0wJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_model()\n",
        "# print(model.summary())"
      ],
      "metadata": {
        "id": "bnphYPbeNKgv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predict_and_save(input_folder, output_folder, model, img_height, img_width)\n",
        "predict_and_save('/content/image_subsets/Kauai_subset/Images/', '/content/Kauai_Predicts/', model, 512, 512)"
      ],
      "metadata": {
        "id": "dlus2_HlMzoP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c666986f-7562-4115-b82c-656b5934602c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 11s 11s/step\n",
            "Saved prediction for 10241_1537.tif\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "Saved prediction for 17921_1537.tif\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Saved prediction for 9217_6145.tif\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "Saved prediction for 9217_8193.tif\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "Saved prediction for 13825_3073.tif\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "Saved prediction for 18433_8705.tif\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "Saved prediction for 8193_10241.tif\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Saved prediction for 10753_6145.tif\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "Saved prediction for 16897_1025.tif\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Saved prediction for 20481_7169.tif\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Saved prediction for 3585_12289.tif\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Saved prediction for 10753_7169.tif\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "Saved prediction for 8193_9729.tif\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "Saved prediction for 13825_11265.tif\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "Saved prediction for 10753_2561.tif\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "Saved prediction for 5633_12801.tif\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "Saved prediction for 8193_3585.tif\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "Saved prediction for 18945_2561.tif\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "Saved prediction for 6657_10753.tif\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "Saved prediction for 14849_8705.tif\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "Saved prediction for 10753_6657.tif\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "Saved prediction for 9217_513.tif\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "Saved prediction for 10753_1025.tif\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Saved prediction for 12801_4609.tif\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "Saved prediction for 14337_8193.tif\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Saved prediction for 11265_4097.tif\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Saved prediction for 8193_7169.tif\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Saved prediction for 16385_8193.tif\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Saved prediction for 10241_8193.tif\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "Saved prediction for 19969_7169.tif\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "Saved prediction for 17409_2561.tif\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Saved prediction for 8705_4097.tif\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Saved prediction for 17921_13825.tif\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Saved prediction for 17921_3073.tif\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "Saved prediction for 8193_15873.tif\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Saved prediction for 8705_8193.tif\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Saved prediction for 18433_8193.tif\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "Saved prediction for 19457_3073.tif\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Saved prediction for 8193_14849.tif\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Saved prediction for 16897_8193.tif\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "Saved prediction for 6145_13825.tif\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "Saved prediction for 14849_12801.tif\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Saved prediction for 14337_11777.tif\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Saved prediction for 18945_12289.tif\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Saved prediction for 8193_5633.tif\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Saved prediction for 17921_13313.tif\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "Saved prediction for 12801_8193.tif\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Saved prediction for 7681_15361.tif\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Saved prediction for 5633_13825.tif\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Saved prediction for 18433_13825.tif\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Saved prediction for 16385_13825.tif\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "Saved prediction for 8705_4609.tif\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "Saved prediction for 12289_6657.tif\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "Saved prediction for 7169_13313.tif\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "Saved prediction for 19457_10753.tif\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "Saved prediction for 20481_6657.tif\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "Saved prediction for 17921_12801.tif\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "Saved prediction for 19457_11265.tif\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "Saved prediction for 9729_7169.tif\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "Saved prediction for 9729_7681.tif\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "Saved prediction for 17921_10753.tif\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Saved prediction for 14849_15873.tif\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "Saved prediction for 19457_5633.tif\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Saved prediction for 14337_12289.tif\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Saved prediction for 16897_9217.tif\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Saved prediction for 16897_12801.tif\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Saved prediction for 12289_4097.tif\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Saved prediction for 19969_2561.tif\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Saved prediction for 18433_10753.tif\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "Saved prediction for 14849_14849.tif\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Saved prediction for 5633_11777.tif\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Saved prediction for 8193_8193.tif\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Saved prediction for 17409_7169.tif\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Saved prediction for 7681_5633.tif\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Saved prediction for 10753_15873.tif\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Saved prediction for 13313_2049.tif\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Saved prediction for 19969_8705.tif\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Saved prediction for 5633_13313.tif\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Saved prediction for 13313_513.tif\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Saved prediction for 16385_9729.tif\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "Saved prediction for 11777_12801.tif\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "Saved prediction for 20993_4609.tif\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Saved prediction for 14849_513.tif\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "Saved prediction for 8705_5633.tif\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Saved prediction for 10753_3073.tif\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Saved prediction for 14849_1025.tif\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Saved prediction for 18433_9217.tif\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Saved prediction for 9217_4609.tif\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "Saved prediction for 18945_7169.tif\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "Saved prediction for 10241_7169.tif\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "Saved prediction for 6145_9729.tif\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "Saved prediction for 12289_16385.tif\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "Saved prediction for 13313_1537.tif\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "Saved prediction for 11777_5121.tif\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "Saved prediction for 18945_11265.tif\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "Saved prediction for 18945_6657.tif\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "Saved prediction for 20481_6145.tif\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "Saved prediction for 17409_11777.tif\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "Saved prediction for 8705_7681.tif\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "Saved prediction for 20993_4097.tif\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Saved prediction for 19457_7169.tif\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "Saved prediction for 16385_9217.tif\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "Saved prediction for 5633_10753.tif\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Saved prediction for 7169_15361.tif\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Saved prediction for 5633_12289.tif\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "Saved prediction for 17921_9217.tif\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Saved prediction for 18433_12801.tif\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Saved prediction for 20481_5121.tif\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Saved prediction for 8705_10241.tif\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Saved prediction for 17921_2561.tif\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Saved prediction for 11265_1537.tif\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Saved prediction for 11777_16385.tif\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Saved prediction for 17409_1025.tif\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Saved prediction for 11777_8193.tif\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "Saved prediction for 20481_4609.tif\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Saved prediction for 15361_1025.tif\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Saved prediction for 18433_3073.tif\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Saved prediction for 15873_14849.tif\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Saved prediction for 20481_5633.tif\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Saved prediction for 12801_1025.tif\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Saved prediction for 10753_3585.tif\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Saved prediction for 7681_15873.tif\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Saved prediction for 20481_4097.tif\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Saved prediction for 10241_7681.tif\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Saved prediction for 7681_5121.tif\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Saved prediction for 9217_7169.tif\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Saved prediction for 19457_9729.tif\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Saved prediction for 11265_7169.tif\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "Saved prediction for 9217_5633.tif\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "Saved prediction for 12289_9217.tif\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "Saved prediction for 12801_1537.tif\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "Saved prediction for 19457_6657.tif\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "Saved prediction for 9217_1025.tif\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "Saved prediction for 15873_8705.tif\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "Saved prediction for 12289_14849.tif\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "Saved prediction for 16897_10241.tif\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "Saved prediction for 10753_1537.tif\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "Saved prediction for 12801_8705.tif\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "Saved prediction for 17921_2049.tif\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Saved prediction for 11777_7681.tif\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "Saved prediction for 13825_6145.tif\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Saved prediction for 15873_3073.tif\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "Saved prediction for 18945_7681.tif\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Saved prediction for 8193_4097.tif\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "Saved prediction for 15873_13313.tif\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "Saved prediction for 7681_12801.tif\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Saved prediction for 13825_13313.tif\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Saved prediction for 19457_2049.tif\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Saved prediction for 7169_11265.tif\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Saved prediction for 8193_4609.tif\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "Saved prediction for 9729_4609.tif\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Saved prediction for 12289_4609.tif\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Saved prediction for 12801_15361.tif\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Saved prediction for 9729_8193.tif\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Saved prediction for 21284_4609.tif\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "Saved prediction for 10241_2049.tif\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Saved prediction for 6657_11265.tif\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Saved prediction for 17921_8705.tif\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "Saved prediction for 9729_1025.tif\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "Saved prediction for 18433_10241.tif\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Saved prediction for 12289_7681.tif\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Saved prediction for 13313_15361.tif\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Saved prediction for 13825_3585.tif\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "Saved prediction for 13825_12801.tif\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Saved prediction for 8705_7169.tif\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "Saved prediction for 14337_14337.tif\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Saved prediction for 12801_5121.tif\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "Saved prediction for 17409_3585.tif\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "Saved prediction for 7681_3585.tif\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "Saved prediction for 11265_7681.tif\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "Saved prediction for 17409_8705.tif\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "Saved prediction for 16897_12289.tif\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "Saved prediction for 14849_10753.tif\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "Saved prediction for 9729_4097.tif\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "Saved prediction for 15361_8193.tif\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "Saved prediction for 10753_4609.tif\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "Saved prediction for 10241_6657.tif\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "Saved prediction for 13825_15361.tif\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "Saved prediction for 11777_4609.tif\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Saved prediction for 16385_8705.tif\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Saved prediction for 12289_5121.tif\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Saved prediction for 9217_5121.tif\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Saved prediction for 17409_7681.tif\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Saved prediction for 8705_1025.tif\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Saved prediction for 9729_5121.tif\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Saved prediction for 12289_8193.tif\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Saved prediction for 14337_3585.tif\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Saved prediction for 14849_9217.tif\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Saved prediction for 19969_5121.tif\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Saved prediction for 18945_6145.tif\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Saved prediction for 5121_10241.tif\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Saved prediction for 15873_9217.tif\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "Saved prediction for 5633_9729.tif\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Saved prediction for 5633_10241.tif\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "Saved prediction for 19969_5633.tif\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "Saved prediction for 18945_11777.tif\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Saved prediction for 12289_1025.tif\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Saved prediction for 8705_5121.tif\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "Saved prediction for 18945_9729.tif\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Saved prediction for 13825_14849.tif\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Saved prediction for 19969_7681.tif\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Saved prediction for 11777_2049.tif\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Saved prediction for 9217_7681.tif\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Saved prediction for 20993_6145.tif\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "Saved prediction for 7681_10241.tif\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "Saved prediction for 9729_6145.tif\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "Saved prediction for 5121_10753.tif\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "Saved prediction for 12289_2049.tif\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "Saved prediction for 16385_15361.tif\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "Saved prediction for 10753_2049.tif\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "Saved prediction for 14337_12801.tif\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "Saved prediction for 7169_4609.tif\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "Saved prediction for 6145_11777.tif\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "Saved prediction for 14337_11265.tif\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "Saved prediction for 17921_11265.tif\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "Saved prediction for 17921_3585.tif\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "Saved prediction for 18433_2561.tif\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "Saved prediction for 3585_11777.tif\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Saved prediction for 17921_8193.tif\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "Saved prediction for 13313_14849.tif\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Saved prediction for 18945_8705.tif\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Saved prediction for 8193_6657.tif\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "Saved prediction for 13825_5633.tif\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "Saved prediction for 7169_10753.tif\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Saved prediction for 18945_9217.tif\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Saved prediction for 19969_8193.tif\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "Saved prediction for 18433_7681.tif\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "Saved prediction for 19457_8193.tif\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Saved prediction for 12289_15873.tif\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Saved prediction for 18945_10753.tif\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "Saved prediction for 6657_14849.tif\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Saved prediction for 7681_4609.tif\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "Saved prediction for 17921_7681.tif\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Saved prediction for 18945_12801.tif\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Saved prediction for 7681_11265.tif\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Saved prediction for 14849_15361.tif\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Saved prediction for 17921_10241.tif\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Saved prediction for 8193_12289.tif\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Saved prediction for 13825_13825.tif\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Saved prediction for 15361_15873.tif\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Saved prediction for 10241_1025.tif\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Saved prediction for 12289_11777.tif\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Saved prediction for 14337_10753.tif\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Saved prediction for 13825_2049.tif\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "Saved prediction for 17409_10241.tif\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "Saved prediction for 12801_7169.tif\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "Saved prediction for 7169_14849.tif\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "Saved prediction for 17921_11777.tif\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "Saved prediction for 513_10241.tif\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "Saved prediction for 17921_9729.tif\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "Saved prediction for 15361_15361.tif\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "Saved prediction for 12289_1537.tif\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "Saved prediction for 9729_15873.tif\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "Saved prediction for 6657_12801.tif\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "Saved prediction for 16897_2049.tif\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "Saved prediction for 10241_2561.tif\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "Saved prediction for 8193_5121.tif\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "Saved prediction for 16897_8705.tif\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Saved prediction for 10753_7681.tif\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Saved prediction for 21284_5633.tif\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Saved prediction for 11777_1537.tif\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Saved prediction for 6657_4609.tif\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Saved prediction for 11265_6657.tif\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Saved prediction for 9217_3585.tif\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Saved prediction for 19969_6657.tif\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Saved prediction for 8193_7681.tif\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Saved prediction for 11777_4097.tif\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "Saved prediction for 14849_14337.tif\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "Saved prediction for 16897_2561.tif\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Saved prediction for 12801_2049.tif\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Saved prediction for 19457_7681.tif\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Saved prediction for 12289_12289.tif\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Saved prediction for 14849_11265.tif\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Saved prediction for 15873_9729.tif\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Saved prediction for 12289_15361.tif\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Saved prediction for 11265_1025.tif\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "Saved prediction for 11265_14337.tif\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Saved prediction for 19969_6145.tif\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Saved prediction for 15361_9217.tif\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Saved prediction for 19457_9217.tif\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Saved prediction for 11777_7169.tif\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "Saved prediction for 15361_1537.tif\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "Saved prediction for 19457_10241.tif\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Saved prediction for 6145_12801.tif\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Saved prediction for 14849_8193.tif\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "Saved prediction for 8705_6657.tif\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "Saved prediction for 15361_12801.tif\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "Saved prediction for 8705_14849.tif\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "Saved prediction for 13313_15873.tif\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "Saved prediction for 9729_5633.tif\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "Saved prediction for 16897_11777.tif\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "Saved prediction for 9217_6657.tif\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "Saved prediction for 19457_8705.tif\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "Saved prediction for 8705_513.tif\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "Saved prediction for 16897_13825.tif\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "Saved prediction for 18433_1537.tif\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Saved prediction for 14337_2561.tif\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Saved prediction for 20993_5633.tif\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "Saved prediction for 5121_11777.tif\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Saved prediction for 13825_12289.tif\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Saved prediction for 15361_11265.tif\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "Saved prediction for 14337_14849.tif\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Saved prediction for 18433_11777.tif\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "Saved prediction for 9729_6657.tif\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Saved prediction for 15361_13825.tif\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Saved prediction for 18433_13313.tif\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Saved prediction for 9217_14849.tif\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Saved prediction for 6657_11777.tif\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "Saved prediction for 14337_4097.tif\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Saved prediction for 7169_4097.tif\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Saved prediction for 9217_3073.tif\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Saved prediction for 17409_13825.tif\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Saved prediction for 18433_7169.tif\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "Saved prediction for 11265_3585.tif\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Saved prediction for 15361_12289.tif\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Saved prediction for 6657_14337.tif\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Saved prediction for 18945_5121.tif\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Saved prediction for 15361_10753.tif\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Saved prediction for 6145_12289.tif\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Saved prediction for 10241_6145.tif\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Saved prediction for 16897_1537.tif\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Saved prediction for 12801_7681.tif\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Saved prediction for 15873_3585.tif\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "Saved prediction for 11777_12289.tif\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "Saved prediction for 19457_5121.tif\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "Saved prediction for 10241_5633.tif\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "Saved prediction for 5121_11265.tif\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "Saved prediction for 6657_12289.tif\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "Saved prediction for 18945_8193.tif\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "Saved prediction for 17921_12289.tif\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "Saved prediction for 7681_4097.tif\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "Saved prediction for 13825_4097.tif\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "Saved prediction for 13825_5121.tif\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "Saved prediction for 15361_8705.tif\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "Saved prediction for 12289_8705.tif\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "Saved prediction for 10753_14849.tif\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "Saved prediction for 13825_2561.tif\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "Saved prediction for 15873_13825.tif\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "Saved prediction for 17409_9729.tif\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "Saved prediction for 17409_13313.tif\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "Saved prediction for 19969_3073.tif\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "Saved prediction for 12289_7169.tif\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "Saved prediction for 17409_11265.tif\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "Saved prediction for 13825_1537.tif\n",
            "Prediction and saving completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sum_df"
      ],
      "metadata": {
        "id": "bPY2fsZ2ZCSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data_gen_test = DataGenerator(sum_df['Images_path'][0:5], sum_df['Labels_path'][0:5], 512, 512, 2, 22)\n",
        "# Create the data generator\n",
        "training_data_generator = DataGenerator(sum_df['Images_path'][sum_df['random_split']==0], sum_df['Labels_path'][sum_df['random_split']==0], 512, 512, 4, 25)\n",
        "validation_data_generator = DataGenerator(sum_df['Images_path'][sum_df['random_split']==1], sum_df['Labels_path'][sum_df['random_split']==1], 512, 512, 4, 25)\n"
      ],
      "metadata": {
        "id": "yMEt7PPpNPlb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_images, batch_labels = training_data_generator.__getitem__(0)"
      ],
      "metadata": {
        "id": "2RhN0onYhUno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_images.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0ivLGGtIMsz",
        "outputId": "7f0d11c3-4e4c-400d-e0f9-49f2a39c97d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 512, 512, 7)"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_labels.shape"
      ],
      "metadata": {
        "id": "KsMavBMDim_6",
        "outputId": "43747874-573a-46ff-d7a1-6e3218abc3b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 512, 512, 25)"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy','categorical_accuracy'])\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"landcover_segmentation.h5\", save_best_only=True)\n",
        "]\n",
        "\n",
        "model.fit(training_data_generator, validation_data=validation_data_generator, epochs=100, callbacks=callbacks,shuffle=True)\n",
        "# model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=16, epochs=10)\n",
        "\n"
      ],
      "metadata": {
        "id": "IG72KHv0Nmuk",
        "outputId": "3e975904-01ba-4ea7-913f-9a862419b362",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 618
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "730/730 [==============================] - ETA: 0s - loss: 1.8993 - accuracy: 0.4109 - categorical_accuracy: 0.4109"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r730/730 [==============================] - 746s 975ms/step - loss: 1.8993 - accuracy: 0.4109 - categorical_accuracy: 0.4109 - val_loss: 1.7933 - val_accuracy: 0.4021 - val_categorical_accuracy: 0.4021\n",
            "Epoch 2/100\n",
            "730/730 [==============================] - 696s 953ms/step - loss: 1.6346 - accuracy: 0.4584 - categorical_accuracy: 0.4584 - val_loss: 1.6206 - val_accuracy: 0.4719 - val_categorical_accuracy: 0.4719\n",
            "Epoch 3/100\n",
            "730/730 [==============================] - 698s 955ms/step - loss: 1.5362 - accuracy: 0.4775 - categorical_accuracy: 0.4775 - val_loss: 1.5800 - val_accuracy: 0.4665 - val_categorical_accuracy: 0.4665\n",
            "Epoch 4/100\n",
            "730/730 [==============================] - 752s 1s/step - loss: 1.4662 - accuracy: 0.4963 - categorical_accuracy: 0.4963 - val_loss: 1.5872 - val_accuracy: 0.4483 - val_categorical_accuracy: 0.4483\n",
            "Epoch 5/100\n",
            "335/730 [============>.................] - ETA: 5:45 - loss: 1.4312 - accuracy: 0.5010 - categorical_accuracy: 0.5010"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-634d30130b02>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m ]\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_data_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m# model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=16, epochs=10)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1740\u001b[0m                         ):\n\u001b[1;32m   1741\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1742\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1743\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1744\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    855\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    856\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 857\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    858\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    146\u001b[0m       (concrete_function,\n\u001b[1;32m    147\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    149\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1347\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1348\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1349\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_call_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1350\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1457\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1458\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1459\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "# history = model.fit(training_data_generator, validation_data=validation_data_generator, epochs=10)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(validation_data_generator)\n",
        "print(\"Validation Loss:\", loss)\n",
        "print(\"Validation Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "egXgb8QoNum4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "outputId": "807543dd-42c5-489b-d279-e14130d751be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "82/82 [==============================] - 66s 804ms/step - loss: 1.6129 - accuracy: 0.4627 - categorical_accuracy: 0.4627\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-697d684c09fe>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Evaluate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_data_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Validation Loss:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Validation Accuracy:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ]
    }
  ]
}