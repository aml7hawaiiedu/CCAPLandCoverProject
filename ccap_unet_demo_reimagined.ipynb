{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aml7hawaiiedu/CCAPLandCoverProject/blob/main/ccap_unet_demo_reimagined.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6-_rOVHH3if"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "This is Amanda's version of UNET_regression_demo_whp (Aron's original code)\n",
        "  \n",
        "This is an Earth Engine <> TenserFlow demonstration notebook. Suppose you want to predict a continuous output (regression) from a stack of continuous inputs. In this example, the output is impervious surface area from [NLCD](https://www.mrlc.gov/data) and the input is a Landsat 8 composite. The model is a [fully convolutional neural network  (FCNN)](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf), specifically [U-net](https://arxiv.org/abs/1505.04597). \n",
        "\n",
        "This version is a reimagination of this original work. I found the reliance on Earth engine functions and tensorflow records to be too difficult for debugging. Here we'll simply export geotiffs to google cloud storage and then move along a more traditional image processing route from there.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "okAzQzK0J4HS"
      },
      "outputs": [],
      "source": [
        "# Google Cloud Platform authentication.\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXQd9K91KLIx",
        "outputId": "154aafa1-d71d-4974-fd9a-85c6ae04ee64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To authorize access needed by Earth Engine, open the following URL in a web browser and follow the instructions. If the web browser does not start automatically, please manually browse the URL below.\n",
            "\n",
            "    https://code.earthengine.google.com/client-auth?scopes=https%3A//www.googleapis.com/auth/earthengine%20https%3A//www.googleapis.com/auth/devstorage.full_control&request_id=O5hkJfc_mxztSU3FZeZTdNlzyNWTXpFuM_AJUDU2amY&tc=w0uSTaXXn1Imu2hRI4jxv1Lt9mLVZA5JjGTDEIP1TFg&cc=q1kqg86OXMJ_0bhX9_HhV0ju3lMuMjlS4s4LBD5L1FY\n",
            "\n",
            "The authorization workflow will generate a code, which you should paste in the box below.\n",
            "Enter verification code: 4/1AVHEtk5nZR3WiXD_UeOq4ondU3zcI7p7wUep932q9a0YWETOJECS-IzrpJU\n",
            "\n",
            "Successfully saved authorization token.\n"
          ]
        }
      ],
      "source": [
        "# Import Google Earth Engine Python API, authenticate user's credentials, and initialize the Earth Engine library.\n",
        "import ee\n",
        "ee.Authenticate()\n",
        "ee.Initialize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-N6SCzKDKc5g",
        "outputId": "173d46f0-e3d2-4215-c68a-ce8471c050a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.12.0\n"
          ]
        }
      ],
      "source": [
        "# Tensorflow is an open-source machine learning library developed by Google. It is wildly used for building deep learning models.\n",
        "# Print the version.\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3qKGJ7evWG7",
        "outputId": "9c0a224d-e95e-4292-acd4-4b884e56deac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.14.0\n"
          ]
        }
      ],
      "source": [
        "# Folium is a Python library used for creating interactive maps and visualizations.\n",
        "# Print the version. \n",
        "import folium\n",
        "print(folium.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "cGuJGj71zX2a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c066d3b-03ce-4cc0-fd5a-b93a485f589d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "remote_sensing_fuckit_bucket\n"
          ]
        }
      ],
      "source": [
        "# Name of the Google Cloud Storage (GCS) bucket where data will be stored or accessed from.\n",
        "# GCS is cloud-based object storage service used to store and access large amounts of data in a highly scalable and durable manner. \n",
        "# It is common to define a bucket which serves as a top-level container. \n",
        "BUCKET = 'remote_sensing_fuckit_bucket'\n",
        "print(BUCKET)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "bWSU2lVyzYVI"
      },
      "outputs": [],
      "source": [
        "# Name of the folder that will be used to store data related to the wetland U-Net model.\n",
        "# Name of the subfolder within 'FOLDER' directory that will be used to store training patches or data used to train the wetland classification model. \n",
        "# Name of the subfolder within 'FOLDER' directory that will be used to store evaluation patches or data used to test the wetland classification model. \n",
        "FOLDER = 'wetland_unet'\n",
        "TRAINING_BASE = 'training_patches'\n",
        "EVAL_BASE = 'eval_patches'\n",
        "\n",
        "# These lines of code specify the input features for the wetland U-Net model and the response variable that the model will be trained to predict.\n",
        "# opticalBands is assigned as a list of string values, which correspond to the names of the optical bands that are present in the Landsat satellite imagery.\n",
        "# thermalBands is assigned a list of string values, which correspond to the names of the optical bands that are present in the Landsat satellite imagery.\n",
        "# BANDS is assigned the concatenation of the 'opticalBands' and 'thermalBands'. This represents a list of all the bands that will be used as features in the U-Net model.\n",
        "# RESPONSE is assigned the string value 'b1' which corresponds to the name of the response variable that the U-Net model will be trained to predict. \n",
        "# FEATURES is assigned the concatenation of the 'BANDS' list and the 'RESPONSE' variables. This represents a list of all the features that will be used to train the U-Net model. \n",
        "opticalBands = ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7']\n",
        "thermalBands = ['B10', 'B11']\n",
        "BANDS = opticalBands + thermalBands\n",
        "RESPONSE = ['p1','p2','p3','p4','p5','p6','p7','p8','p9','p10','p11','p12','p13','p14','p15','p16','p17','p18','p19','p20','p21','p22','p23','p24','p25']\n",
        "FEATURES = BANDS + RESPONSE\n",
        "# Specify the inputs\n",
        "# These lines of code specify the size and shape of patches that the U-Net model expects as input, as well as a dictionary that maps featue names to columns in an TenserFlow dataset. \n",
        "# KERNEL_SIZE is assigned the integer value '128' which represents the size of patches that the U-Net model expects as input.\n",
        "# KERNEL_SHAPE is assigned a list contning two elements, both of which are equal to 'KERNEL_SIZE'. This represents the shape of the patches the U-Net model expects as an input.\n",
        "# COLUMNS is assigned a list of TenserFlow 'fixedLenFeature' objects, which specify the expected shape and data type of each feature column in a Tenserflow dataset. \n",
        "  # 'shape' is set to 'KERNEL_SHAPE' and 'dtype' is set to 'tf.float32'. The list comprehension used here creates one 'fixedLenFeature' object for each feature in the 'FEATURES' list/ \n",
        "# FEATURE_DICT is assigned a Python dictionary that maps feature names to their corresponding columns in a TenserFlow dataset. \n",
        "  # 'zip' is used to create tuples parining each feature name from the 'FEATURES' list with its corresponding 'fixedLenFeature' object from the 'COLUMNS' list. \n",
        "  # The 'dict' function is then used to convert this list of tuples into a dictionary. \n",
        "KERNEL_SIZE = 128\n",
        "KERNEL_SHAPE = [KERNEL_SIZE, KERNEL_SIZE]\n",
        "COLUMNS = [\n",
        "  tf.io.FixedLenFeature(shape=KERNEL_SHAPE, dtype=tf.float32) for k in FEATURES\n",
        "]\n",
        "FEATURES_DICT = dict(zip(FEATURES, COLUMNS))\n",
        "\n",
        "# Define the size of the training and evaluation datasets that will be used to train and evaluate the U-Net model. \n",
        "# TRAIN_SIZE is assigned the integer value '1600' which represents the size of the training dataset.\n",
        "# EVAL_SIZE is assigned the integer value '800' which represents the size of the evaluation dataset. \n",
        "# TRAIN_SIZE = 1600\n",
        "# EVAL_SIZE = 800\n",
        "\n",
        "# Specify model training parameters.\n",
        "# Define model training parameters that will be used to train and evaluate the U-Net model. \n",
        "# BATCH_SIZE represents the number of training examples that will be used in each training iteration. \n",
        "# EPOCHS represents the number of times that each training dataset will be used to train the model. \n",
        "# BUFFER_SIZE represents the number of elements from the training dataset that will be pre-fetched and buffered in order to optimize data reading during training. \n",
        "# OPTIMIZER specifies the Stochastic Gradient Descent optimization algorithm will be used during training. \n",
        "  # ? what other optimizers would work? Is this the best one for this type of model?\n",
        "# LOSS specifies the Mean Squared Error Loss function will be used during training. \n",
        "# METRICS is asigned a list containing the sting value 'RootMeanSquareError' which specifies RMSE metric will be used to evaluate the performance of the model during training. \n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 10\n",
        "# BUFFER_SIZE = 2000\n",
        "OPTIMIZER = 'adam'\n",
        "LOSS = 'CategoricalCrossentropy'\n",
        "METRICS = ['acc']\n",
        "\n",
        "\n",
        "aoiPolys = ee.FeatureCollection('projects/ee-seismosmsr-landcover/assets/hawaii_pos_128_3840m_singleparts') "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ee\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "def exportImagesToGCS(image, aoiPolys, bucket_name, file_prefix):\n",
        "    \"\"\"Export an Earth Engine image to a GCS bucket as a GeoTIFF for each polygon in a set of polygons.\n",
        "    \n",
        "    Args:\n",
        "        image (ee.Image): The Earth Engine image to export.\n",
        "        aoiPolys (ee.FeatureCollection): The set of polygons defining the AOI.\n",
        "        bucket_name (str): The name of the GCS bucket to export the images to.\n",
        "        file_prefix (str): The prefix to use for the file names of the exported images.\n",
        "    \"\"\"\n",
        "    # Define the GCS URI for the output files.\n",
        "    output_base_uri = 'gs://{}/{}'.format(bucket_name, file_prefix)\n",
        "    \n",
        "    # Cast all bands to Float32 to ensure compatibility.\n",
        "    image = image.toFloat()\n",
        "    \n",
        "    # Loop over the polygons and export one image for each polygon.\n",
        "    for i, poly in enumerate(aoiPolys.getInfo()['features']):\n",
        "        poly_name = poly['properties'].get('name', str(i))\n",
        "        output_uri = '{}/{}.tif'.format(output_base_uri, poly_name)\n",
        "        \n",
        "        # Export the image to the GCS bucket as a GeoTIFF.\n",
        "        task = ee.batch.Export.image.toCloudStorage(\n",
        "            image=image.clip(poly['geometry']),\n",
        "            description='Export {} to GCS'.format(poly_name),\n",
        "            bucket=bucket_name,\n",
        "            fileNamePrefix='{}/{}'.format(file_prefix, poly_name),\n",
        "            scale=30,\n",
        "            maxPixels=1e13,\n",
        "            fileFormat='GeoTIFF',\n",
        "        )\n",
        "        task.start()\n",
        "        \n",
        "        # # Print the status while waiting for the export to complete.\n",
        "        # while task.status()['state'] in ['READY', 'RUNNING']:\n",
        "        #     print('Exporting {} to GCS: {}...'.format(poly_name, task.status()['description']), end='\\r')\n",
        "        #     time.sleep(30)\n",
        "        \n",
        "        # # Check if the export succeeded.\n",
        "        # if task.status()['state'] == 'COMPLETED':\n",
        "        #     print('Export {} to GCS succeeded.'.format(poly_name))\n",
        "        # else:\n",
        "        #     print('Export {} to GCS failed.'.format(poly_name))\n",
        "            \n",
        "        # Set the GCS object's ACL to be publicly accessible.\n",
        "        # gsutil_path = !which gsutil\n",
        "        # acl_cmd = '{} acl ch -u AllUsers:R {}'.format(gsutil_path[0], output_uri)\n",
        "        # subprocess.check_output(acl_cmd.split())"
      ],
      "metadata": {
        "id": "We0l3zeAKcDs"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 796
        },
        "id": "tnLLLqcmzeVR",
        "outputId": "d35874ac-7ae8-4597-b795-8a90951c8773"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<folium.folium.Map at 0x7fe21bc42400>"
            ],
            "text/html": [
              "<div style=\"width:100%;\"><div style=\"position:relative;width:100%;height:0;padding-bottom:60%;\"><span style=\"color:#565656\">Make this Notebook Trusted to load map: File -> Trust Notebook</span><iframe srcdoc=\"&lt;!DOCTYPE html&gt;\n",
              "&lt;html&gt;\n",
              "&lt;head&gt;\n",
              "    \n",
              "    &lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html; charset=UTF-8&quot; /&gt;\n",
              "    \n",
              "        &lt;script&gt;\n",
              "            L_NO_TOUCH = false;\n",
              "            L_DISABLE_3D = false;\n",
              "        &lt;/script&gt;\n",
              "    \n",
              "    &lt;style&gt;html, body {width: 100%;height: 100%;margin: 0;padding: 0;}&lt;/style&gt;\n",
              "    &lt;style&gt;#map {position:absolute;top:0;bottom:0;right:0;left:0;}&lt;/style&gt;\n",
              "    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.js&quot;&gt;&lt;/script&gt;\n",
              "    &lt;script src=&quot;https://code.jquery.com/jquery-1.12.4.min.js&quot;&gt;&lt;/script&gt;\n",
              "    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/js/bootstrap.bundle.min.js&quot;&gt;&lt;/script&gt;\n",
              "    &lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.js&quot;&gt;&lt;/script&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/css/bootstrap.min.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://netdna.bootstrapcdn.com/bootstrap/3.0.0/css/bootstrap.min.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.2.0/css/all.min.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/gh/python-visualization/folium/folium/templates/leaflet.awesome.rotate.min.css&quot;/&gt;\n",
              "    \n",
              "            &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width,\n",
              "                initial-scale=1.0, maximum-scale=1.0, user-scalable=no&quot; /&gt;\n",
              "            &lt;style&gt;\n",
              "                #map_101f6a7e36fc1b07a060f97ac10dbe4f {\n",
              "                    position: relative;\n",
              "                    width: 100.0%;\n",
              "                    height: 100.0%;\n",
              "                    left: 0.0%;\n",
              "                    top: 0.0%;\n",
              "                }\n",
              "                .leaflet-container { font-size: 1rem; }\n",
              "            &lt;/style&gt;\n",
              "        \n",
              "&lt;/head&gt;\n",
              "&lt;body&gt;\n",
              "    \n",
              "    \n",
              "            &lt;div class=&quot;folium-map&quot; id=&quot;map_101f6a7e36fc1b07a060f97ac10dbe4f&quot; &gt;&lt;/div&gt;\n",
              "        \n",
              "&lt;/body&gt;\n",
              "&lt;script&gt;\n",
              "    \n",
              "    \n",
              "            var map_101f6a7e36fc1b07a060f97ac10dbe4f = L.map(\n",
              "                &quot;map_101f6a7e36fc1b07a060f97ac10dbe4f&quot;,\n",
              "                {\n",
              "                    center: [21.4, -158.0],\n",
              "                    crs: L.CRS.EPSG3857,\n",
              "                    zoom: 10,\n",
              "                    zoomControl: true,\n",
              "                    preferCanvas: false,\n",
              "                }\n",
              "            );\n",
              "\n",
              "            \n",
              "\n",
              "        \n",
              "    \n",
              "            var tile_layer_4e50beda73af5532ac36522ed9eccb1c = L.tileLayer(\n",
              "                &quot;https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png&quot;,\n",
              "                {&quot;attribution&quot;: &quot;Data by \\u0026copy; \\u003ca target=\\&quot;_blank\\&quot; href=\\&quot;http://openstreetmap.org\\&quot;\\u003eOpenStreetMap\\u003c/a\\u003e, under \\u003ca target=\\&quot;_blank\\&quot; href=\\&quot;http://www.openstreetmap.org/copyright\\&quot;\\u003eODbL\\u003c/a\\u003e.&quot;, &quot;detectRetina&quot;: false, &quot;maxNativeZoom&quot;: 18, &quot;maxZoom&quot;: 18, &quot;minZoom&quot;: 0, &quot;noWrap&quot;: false, &quot;opacity&quot;: 1, &quot;subdomains&quot;: &quot;abc&quot;, &quot;tms&quot;: false}\n",
              "            ).addTo(map_101f6a7e36fc1b07a060f97ac10dbe4f);\n",
              "        \n",
              "    \n",
              "            var tile_layer_6b58163f4526abbbcd625eb66b67f977 = L.tileLayer(\n",
              "                &quot;https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/maps/dc759d70ea8d9631f53c68fab4dcc9ce-dbf9af2e054f5748c4fdedfbc3176169/tiles/{z}/{x}/{y}&quot;,\n",
              "                {&quot;attribution&quot;: &quot;Map Data \\u0026copy; \\u003ca href=\\&quot;https://earthengine.google.com/\\&quot;\\u003eGoogle Earth Engine\\u003c/a\\u003e&quot;, &quot;detectRetina&quot;: false, &quot;maxNativeZoom&quot;: 18, &quot;maxZoom&quot;: 18, &quot;minZoom&quot;: 0, &quot;noWrap&quot;: false, &quot;opacity&quot;: 1, &quot;subdomains&quot;: &quot;abc&quot;, &quot;tms&quot;: false}\n",
              "            ).addTo(map_101f6a7e36fc1b07a060f97ac10dbe4f);\n",
              "        \n",
              "&lt;/script&gt;\n",
              "&lt;/html&gt;\" style=\"position:absolute;width:100%;height:100%;left:0;top:0;border:none !important;\" allowfullscreen webkitallowfullscreen mozallowfullscreen></iframe></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ],
      "source": [
        "# Use Landsat 8 surface reflectance data.\n",
        "# Use EE Python API to initialize an image collection. \n",
        "# Specify Landsat 8 surface reflectance image collection. \n",
        "# l8sr is the name of the object to be used in subsequent operations.\n",
        "\n",
        "l8sr = ee.ImageCollection('LANDSAT/LC08/C01/T1_SR')\n",
        "\n",
        "# This code defines a function 'maskL8sr' that will be used to apply cloud masking to Landsat 8 surface reflectance images.\n",
        "def maskL8sr(image):\n",
        "  cloudShadowBitMask = ee.Number(2).pow(3).int()\n",
        "  cloudsBitMask = ee.Number(2).pow(5).int()\n",
        "  qa = image.select('pixel_qa')\n",
        "  mask1 = qa.bitwiseAnd(cloudShadowBitMask).eq(0).And(\n",
        "    qa.bitwiseAnd(cloudsBitMask).eq(0))\n",
        "  mask2 = image.mask().reduce('min')\n",
        "  mask3 = image.select(opticalBands).gt(0).And(\n",
        "          image.select(opticalBands).lt(10000)).reduce('min')\n",
        "  mask = mask1.And(mask2).And(mask3)\n",
        "  return image.select(opticalBands).divide(10000).addBands(\n",
        "          image.select(thermalBands).divide(10).clamp(273.15, 373.15)\n",
        "            .subtract(273.15).divide(100)).updateMask(mask)\n",
        "\n",
        "\n",
        "\n",
        "# The image input data is a cloud-masked median composite.\n",
        "# Filter the l8sr image collection by date, selecting only images from the date range identified. \n",
        "# Apply the mask function to each image in the resulting collection using the map() method. \n",
        "  # This function applies a mask to each image and rescales the reflectance and temperature bands to a 0-1 range. \n",
        "# Calculate the median value of the resulting image collection using the median() method.\n",
        "# Assign the variable name; in this case 'ls_2017'. \n",
        "# The resulting 'ls_2017' image will be used for training and evaluation of our U-Net model. \n",
        "\n",
        "ls_2017 = l8sr.filterDate('2017-01-01', '2017-12-30').map(maskL8sr).median().clip(aoiPolys.geometry()).toFloat() \n",
        "\n",
        "# Replace NaN values with 0.\n",
        "# nan_mask = ls_2017.mask(ls_2017)\n",
        "# image_with_zero  = ls_2017.updateMask(nan_mask).unmask(0)\n",
        "\n",
        "# Create single mosaic\n",
        "#ls_3yr_mosaic =  ls_2017.addBands(ls_2018)\n",
        "#ls_3yr_mosaic = ls_3yr_mosaic.addBands(ls_2019)\n",
        "\n",
        "#Reset band names to new stack\n",
        "#BANDS = ls_3yr_mosaic.bandNames().getInfo()\n",
        "#FEATURES = BANDS + [RESPONSE]\n",
        "\n",
        "# Use folium to visualize the imagery.\n",
        "# map = folium.Map(location=[21.4, -158.])\n",
        "\n",
        "\n",
        "\n",
        "# Use 'getMapID()' method to get a map ID for the 'ls_2017' image. \n",
        "# Specify that we want to visualize band 2 of the image using 'bands':['B2']. Set the min and max.\n",
        "# Create a folium map object centered at specific location using 'folium.Map()'. \n",
        "# Add a tile layer to the map using 'folium.TileLayer()' method, specifying the URL format of the tiles using the 'tiles' argument and adding 'attr' attribution info. \n",
        "# Set the overlay flag to 'True' and provide a name for the layer using 'name'. \n",
        "# Finally, add the tile layer to the map using '.add_to()' method. \n",
        "  # This allows us to visualize the 'ls_2017' image in the folium map. \n",
        "mapid_17 =ls_2017.getMapId({'bands': ['B2','B3','B4'], 'min': 0, 'max':.2})\n",
        "map = folium.Map(location=[21.4, -158.])\n",
        "folium.TileLayer(\n",
        "    tiles=mapid_17['tile_fetcher'].url_format,\n",
        "    attr = 'Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name = '17 median composite',\n",
        "    ).add_to(map)\n",
        "\n",
        "map"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# aoiPolys.getInfo()['features'][1]"
      ],
      "metadata": {
        "id": "EEDIv60uPcLP"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "I4X7EyYwv6m0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96325cd7-cfcc-4eea-a51d-0a3082679fd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<function oneHot at 0x7fe21c334f70>\n"
          ]
        }
      ],
      "source": [
        "# One Hot Encoding function.\n",
        "# 'oneHot()' function is used for One Hot Encoding of an image. \n",
        "  # Each pixel in the image is replaced by a binary vector of length 'n' where 'n' is the number of classes in the image. \n",
        "# image : image to be encoded\n",
        "# code : the class code that will be encoded as '1' in the output, while all other classes are encoded as '0'\n",
        "# The function compares each pixel value in the input 'image' with the 'code' value using the 'ee.image.eq()' finction. \n",
        "# The 'ee.image.eq()' function returns a binary image with the same dimensions as the input image, where each pixel is '1' if the input pixel is equal to the given 'code' value and '0' otherwise. \n",
        "# The resulting binary image is returned as the 'one_hot_image' encoded. \n",
        "def oneHot(image,code = 1):\n",
        "  one_hot_image = image.eq(code)\n",
        "  return one_hot_image\n",
        "  \n",
        "print(oneHot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 796
        },
        "id": "KxvKSAGh_7D3",
        "outputId": "3e67f1bc-2393-467b-8da7-7bf87196ab64"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<folium.folium.Map at 0x7fe21bbee0d0>"
            ],
            "text/html": [
              "<div style=\"width:100%;\"><div style=\"position:relative;width:100%;height:0;padding-bottom:60%;\"><span style=\"color:#565656\">Make this Notebook Trusted to load map: File -> Trust Notebook</span><iframe srcdoc=\"&lt;!DOCTYPE html&gt;\n",
              "&lt;html&gt;\n",
              "&lt;head&gt;\n",
              "    \n",
              "    &lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html; charset=UTF-8&quot; /&gt;\n",
              "    \n",
              "        &lt;script&gt;\n",
              "            L_NO_TOUCH = false;\n",
              "            L_DISABLE_3D = false;\n",
              "        &lt;/script&gt;\n",
              "    \n",
              "    &lt;style&gt;html, body {width: 100%;height: 100%;margin: 0;padding: 0;}&lt;/style&gt;\n",
              "    &lt;style&gt;#map {position:absolute;top:0;bottom:0;right:0;left:0;}&lt;/style&gt;\n",
              "    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.js&quot;&gt;&lt;/script&gt;\n",
              "    &lt;script src=&quot;https://code.jquery.com/jquery-1.12.4.min.js&quot;&gt;&lt;/script&gt;\n",
              "    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/js/bootstrap.bundle.min.js&quot;&gt;&lt;/script&gt;\n",
              "    &lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.js&quot;&gt;&lt;/script&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/css/bootstrap.min.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://netdna.bootstrapcdn.com/bootstrap/3.0.0/css/bootstrap.min.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.2.0/css/all.min.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/gh/python-visualization/folium/folium/templates/leaflet.awesome.rotate.min.css&quot;/&gt;\n",
              "    \n",
              "            &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width,\n",
              "                initial-scale=1.0, maximum-scale=1.0, user-scalable=no&quot; /&gt;\n",
              "            &lt;style&gt;\n",
              "                #map_8bd53b4cb95176d3d707aeac2ff0a9b8 {\n",
              "                    position: relative;\n",
              "                    width: 100.0%;\n",
              "                    height: 100.0%;\n",
              "                    left: 0.0%;\n",
              "                    top: 0.0%;\n",
              "                }\n",
              "                .leaflet-container { font-size: 1rem; }\n",
              "            &lt;/style&gt;\n",
              "        \n",
              "&lt;/head&gt;\n",
              "&lt;body&gt;\n",
              "    \n",
              "    \n",
              "            &lt;div class=&quot;folium-map&quot; id=&quot;map_8bd53b4cb95176d3d707aeac2ff0a9b8&quot; &gt;&lt;/div&gt;\n",
              "        \n",
              "&lt;/body&gt;\n",
              "&lt;script&gt;\n",
              "    \n",
              "    \n",
              "            var map_8bd53b4cb95176d3d707aeac2ff0a9b8 = L.map(\n",
              "                &quot;map_8bd53b4cb95176d3d707aeac2ff0a9b8&quot;,\n",
              "                {\n",
              "                    center: [21.4, -158.0],\n",
              "                    crs: L.CRS.EPSG3857,\n",
              "                    zoom: 10,\n",
              "                    zoomControl: true,\n",
              "                    preferCanvas: false,\n",
              "                }\n",
              "            );\n",
              "\n",
              "            \n",
              "\n",
              "        \n",
              "    \n",
              "            var tile_layer_442fc142e642bb86231e02494c7e03e1 = L.tileLayer(\n",
              "                &quot;https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png&quot;,\n",
              "                {&quot;attribution&quot;: &quot;Data by \\u0026copy; \\u003ca target=\\&quot;_blank\\&quot; href=\\&quot;http://openstreetmap.org\\&quot;\\u003eOpenStreetMap\\u003c/a\\u003e, under \\u003ca target=\\&quot;_blank\\&quot; href=\\&quot;http://www.openstreetmap.org/copyright\\&quot;\\u003eODbL\\u003c/a\\u003e.&quot;, &quot;detectRetina&quot;: false, &quot;maxNativeZoom&quot;: 18, &quot;maxZoom&quot;: 18, &quot;minZoom&quot;: 0, &quot;noWrap&quot;: false, &quot;opacity&quot;: 1, &quot;subdomains&quot;: &quot;abc&quot;, &quot;tms&quot;: false}\n",
              "            ).addTo(map_8bd53b4cb95176d3d707aeac2ff0a9b8);\n",
              "        \n",
              "    \n",
              "            var tile_layer_6b2cea8c12795237ae47419243392849 = L.tileLayer(\n",
              "                &quot;https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/maps/b5f77f4acefe21e2bceb9fb3bd5ae3ae-1f0486f1b73782ee6df7391a90c08917/tiles/{z}/{x}/{y}&quot;,\n",
              "                {&quot;attribution&quot;: &quot;Map Data \\u0026copy; \\u003ca href=\\&quot;https://earthengine.google.com/\\&quot;\\u003eGoogle Earth Engine\\u003c/a\\u003e&quot;, &quot;detectRetina&quot;: false, &quot;maxNativeZoom&quot;: 18, &quot;maxZoom&quot;: 18, &quot;minZoom&quot;: 0, &quot;noWrap&quot;: false, &quot;opacity&quot;: 1, &quot;subdomains&quot;: &quot;abc&quot;, &quot;tms&quot;: false}\n",
              "            ).addTo(map_8bd53b4cb95176d3d707aeac2ff0a9b8);\n",
              "        \n",
              "    \n",
              "            var layer_control_63ce5c31d5e808ff2f67969bb6c4e296 = {\n",
              "                base_layers : {\n",
              "                    &quot;openstreetmap&quot; : tile_layer_442fc142e642bb86231e02494c7e03e1,\n",
              "                },\n",
              "                overlays :  {\n",
              "                    &quot;NOAA CCAP Land Cover&quot; : tile_layer_6b2cea8c12795237ae47419243392849,\n",
              "                },\n",
              "            };\n",
              "            L.control.layers(\n",
              "                layer_control_63ce5c31d5e808ff2f67969bb6c4e296.base_layers,\n",
              "                layer_control_63ce5c31d5e808ff2f67969bb6c4e296.overlays,\n",
              "                {&quot;autoZIndex&quot;: true, &quot;collapsed&quot;: true, &quot;position&quot;: &quot;topright&quot;}\n",
              "            ).addTo(map_8bd53b4cb95176d3d707aeac2ff0a9b8);\n",
              "        \n",
              "&lt;/script&gt;\n",
              "&lt;/html&gt;\" style=\"position:absolute;width:100%;height:100%;left:0;top:0;border:none !important;\" allowfullscreen webkitallowfullscreen mozallowfullscreen></iframe></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ],
      "source": [
        "# get imagecollection from google earth engine - NOAA C-CAP land cover data for Hawaii \n",
        "# This code gets an EE image collection which contains the land cover data for Hawaii form the NOAA Coastal Change Analysis Program (C-CAP)\n",
        "wetland = ee.ImageCollection(\"projects/sat-io/open-datasets/HRLC/CCAP_HI_LC\")\n",
        "\n",
        "# Filter image collection to include images within a specified date range. \n",
        "# The 'mosaic()' function is used to combine all the images in the filtered collection into a single image, which represents the most recent pixel value at each location. \n",
        "# This is done to reduce the numer of images in the collection and to create a single image for analysis. \n",
        "wetland = wetland.filterDate('2010-01-01', '2022-01-01').mosaic().clip(aoiPolys.geometry())\n",
        "\n",
        "# Mask out pixels with a value of 0 in the 'wetland' image using the mask() function.\n",
        "# This ensures the modeld doesn't predict or train on data outside of the acutual land cover area. \n",
        "# wetland = wetland.mask(wetland.gt(0))\n",
        "\n",
        "# This generates a one-hot encoding representation of the land cover classification image for Hawaii obtained from the NOAA C-CAP dataset.\n",
        "# For each of the 25 land cover classes present in the image, the code creates a binary image where pixels with the corresponding class are set to 1 and pixels with other classes are set to 0.\n",
        "# The reutning binary iamges are stored in a list 'wetland_oneHot'.\n",
        "# The list is converted to a single multi-band image.\n",
        "# The bands of the multi-image are renamed to correspond to the class lables.\n",
        "# [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25]\n",
        "# ['p1','p2','p3','p4','p5','p6','p7','p8','p9','p10','p11','p12','p13','p14','p15','p16','p17','p18','p19','p20','p21','p22','p23','p24','p25']\n",
        "wetland_oneHot = []\n",
        "for code in range(len(RESPONSE)):\n",
        "  one_hot_image = oneHot(wetland,code)\n",
        "  wetland_oneHot.append(one_hot_image)\n",
        "wetland_oneHot = ee.Image(wetland_oneHot)\n",
        "wetland_oneHot = wetland_oneHot.rename(RESPONSE)\n",
        "wetland_oneHot = wetland_oneHot.toInt()\n",
        "# Use folium to visualize the imagery.\n",
        "# The 'wetland_oneHot' image is used to generate the map ID. \n",
        "# The min and max are set to 0 and 1 to display the binary values of the one-hot encoding. \n",
        "mapid = wetland_oneHot.getMapId({'bands': ['p10','p15','p11'],'min': 0, 'max': 1})\n",
        "map = folium.Map(location=[21.4, -158])\n",
        "folium.TileLayer(\n",
        "    tiles=mapid['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='NOAA CCAP Land Cover',\n",
        "  ).add_to(map)\n",
        "map.add_child(folium.LayerControl())\n",
        "map"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "exportImagesToGCS(ls_2017,aoiPolys, BUCKET,'hawaii_2017_ls' )"
      ],
      "metadata": {
        "id": "6IwkqFxYa_6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exportImagesToGCS(wetland,aoiPolys, BUCKET,'hawaii_2017_wetlands' )"
      ],
      "metadata": {
        "id": "zdxR2jKJKHIh"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exportImagesToGCS(wetland_oneHot,aoiPolys, BUCKET,'hawaii_2017_wetlands_oneHot' )"
      ],
      "metadata": {
        "id": "uYBYumxKXTu2"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEk2QDx1_66B"
      },
      "source": [
        "Stack the 2D images (Landsat  and NOAA C-CAP land cover data for Hawaii) to create a single image from which samples can be taken. \n",
        "\n",
        "Convert the image into an array image in which each pixel stores 128x128 patches of pixels for each band. \n",
        "\n",
        "This is a key step that bears emphasis: to export training patches, convert a multi-band image to an array image using neighborhoodToArray(), then sample the image at points."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install rasterio"
      ],
      "metadata": {
        "id": "mDO_pGt2cCPg"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remote_sensing_fuckit_bucket/hawaii_2017_ls/1.tif\n",
        "import rasterio\n",
        "from rasterio import warp\n",
        "from skimage.util.shape import view_as_blocks\n",
        "# Define the GCP bucket and filename\n",
        "BUCKET_NAME = 'remote_sensing_fuckit_bucket'\n",
        "BANDS_PATH = 'hawaii_2017_ls/1.tif'\n",
        "CLASS_PATH = 'hawaii_2017_wetlands/1.tif'\n",
        "\n",
        "# Set the block shape (i.e., the size of the image chips)\n",
        "BLOCK_SHAPE = (128, 128)\n",
        "\n",
        "# Open the raster images using rasterio and Google Cloud Storage\n",
        "with rasterio.Env(GS_BUCKET=BUCKET_NAME):\n",
        "    with rasterio.open(f'/vsigs/{BUCKET_NAME}/{BANDS_PATH}') as dataset_bands, rasterio.open(f'/vsigs/{BUCKET_NAME}/{CLASS_PATH}') as dataset_class:\n",
        "  # Read the image data into numpy arrays\n",
        "        # Read the image data into numpy arrays\n",
        "        bands = dataset_bands.read().astype(np.float32)\n",
        "        classification = dataset_class.read()\n",
        "\n",
        "\n",
        "        print(bands.shape)\n",
        "        print(classification.shape)\n",
        "\n",
        "        num_bands, height, width  = bands.shape\n",
        "\n",
        "        clipped_shape_bands = (num_bands,\n",
        "                              BLOCK_SHAPE[0] * int(np.floor(height / BLOCK_SHAPE[0])),\n",
        "                              BLOCK_SHAPE[1] * int(np.floor(width / BLOCK_SHAPE[1])))\n",
        "        \n",
        "        bands = bands[:clipped_shape_bands[0], :clipped_shape_bands[1],:clipped_shape_bands[2]]\n",
        "\n",
        "\n",
        "        num_class, height, width  = classification.shape\n",
        "\n",
        "        clipped_shape_class = (num_class,\n",
        "                              BLOCK_SHAPE[0] * int(np.floor(height / BLOCK_SHAPE[0])),\n",
        "                              BLOCK_SHAPE[1] * int(np.floor(width / BLOCK_SHAPE[1])))\n",
        "\n",
        "        \n",
        "        classification = classification[:clipped_shape_class[0], :clipped_shape_class[1],:clipped_shape_class[2]]\n",
        "        print(clipped_shape_class)\n",
        "        print(clipped_shape_bands)\n",
        "        # print(bands.shape)\n",
        "        # print(classification.shape)\n",
        "\n",
        "        # Use view_as_blocks to extract the image chips\n",
        "        ys, xs = np.indices(bands[..., 0].shape, dtype=np.float32)\n",
        "\n",
        "        # bands_chips = view_as_blocks(bands, (*clipped_shape_bands[0:1], num_bands)).reshape(-1, *clipped_shape_bands[0:1], num_bands)\n",
        "\n",
        "        bands_chips = view_as_blocks(bands, (num_bands,*BLOCK_SHAPE)).reshape(-1, *BLOCK_SHAPE,num_bands)\n",
        "\n",
        "        class_chips = view_as_blocks(classification, (1,*BLOCK_SHAPE)).reshape(-1, *BLOCK_SHAPE)\n",
        "\n",
        "\n",
        "        # Normalize the image chips so that pixel values are between 0 and 1\n",
        "        # bands_chips /= 255.0\n",
        "\n",
        "        # (Optional) Resize the image chips to a specific size\n",
        "        # bands_chips = tf.image.resize(bands_chips, (224, 224))\n",
        "\n",
        "        # Convert the classification chips to one-hot encoded format\n",
        "        class_chips = tf.one_hot(class_chips, depth=25)\n",
        "\n",
        "# Print the shape of the image chips arrays\n",
        "print('Bands chips shape:', bands_chips.shape)\n",
        "print('Class chips shape:', class_chips.shape)"
      ],
      "metadata": {
        "id": "4hf5UyjS1JPX",
        "outputId": "1131bfc4-423b-47b9-e51e-4b831f4d193d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(9, 1926, 2926)\n",
            "(1, 1926, 2926)\n",
            "(1, 1920, 2816)\n",
            "(9, 1920, 2816)\n",
            "Bands chips shape: (330, 128, 128, 9)\n",
            "Class chips shape: (330, 128, 128, 25)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  # Set the proportion of non-zero pixels required to keep a chip\n",
        "  min_prop = 0.1\n",
        "\n",
        "\n",
        "  # print()\n",
        "  keep_index = [i for i, chip in enumerate(bands_chips) if np.sum(np.isnan(chip))/np.size(chip) < min_prop]\n",
        "\n",
        "  subset_bands_chips = bands_chips[keep_index,:,:,:]\n",
        "  subset_bands_chips = np.nan_to_num(subset_bands_chips)\n",
        "  print(subset_bands_chips.shape)\n",
        "  # class_chips )\n",
        "  subset_class_chips = class_chips.numpy()[keep_index,:,:,:]\n",
        "  subset_class_chips = np.nan_to_num(subset_class_chips)\n",
        "  print(subset_class_chips.shape)\n"
      ],
      "metadata": {
        "id": "yQL8vCYodN1R",
        "outputId": "326aace7-da05-4cf9-a610-f1809f1bcdf2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 207,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(169, 128, 128, 9)\n",
            "(169, 128, 128, 25)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def write_tfrecords_to_gcs(bucket_name, images, labels, train_fraction, output_dir):\n",
        "    # Split data into train and test sets\n",
        "    num_samples = len(images)\n",
        "    num_train_samples = int(num_samples * train_fraction)\n",
        "    num_test_samples = num_samples - num_train_samples\n",
        "    train_images, train_labels = images[:num_train_samples], labels[:num_train_samples]\n",
        "    test_images, test_labels = images[num_train_samples:], labels[num_train_samples:]\n",
        "\n",
        "    # Define function to serialize examples\n",
        "    def serialize_example(image, label):\n",
        "        feature = {\n",
        "            'image': tf.train.Feature(float_list=tf.train.FloatList(value=image.ravel())),\n",
        "            'label': tf.train.Feature(int64_list=tf.train.Int64List(value=[label])),\n",
        "        }\n",
        "        example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
        "        return example_proto.SerializeToString()\n",
        "\n",
        "    # Write train and test sets to TFRecord files\n",
        "    for name, images, labels in [('train', train_images, train_labels), ('test', test_images, test_labels)]:\n",
        "        output_path = os.path.join(output_dir, '{}.tfrecords'.format(name))\n",
        "        with tf.io.TFRecordWriter(output_path) as writer:\n",
        "            for i, (image, label) in enumerate(zip(images, labels)):\n",
        "                example = serialize_example(image, label)\n",
        "                writer.write(example)\n",
        "                print('Wrote example {} to {}'.format(i+1, output_path))\n",
        "\n",
        "    # Copy TFRecord files to GCS bucket\n",
        "    for file_name in os.listdir(output_dir):\n",
        "        if file_name.endswith('.tfrecords'):\n",
        "            file_path = os.path.join(output_dir, file_name)\n",
        "            blob_name = os.path.join('tfrecords', file_name)\n",
        "            destination_blob_name = os.path.join('gs://', bucket_name, blob_name)\n",
        "            os.system('gsutil cp {} {}'.format(file_path, destination_blob_name))\n",
        "            print('Copied {} to GCS bucket {}'.format(file_path, bucket_name))\n"
      ],
      "metadata": {
        "id": "D1ibiwyVx8GK"
      },
      "execution_count": 209,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BUCKET_NAME = 'your-bucket-name'\n",
        "TRAIN_FRACTION = 0.8\n",
        "OUTPUT_DIR = 'path/to/output/dir'\n",
        "\n",
        "write_tfrecords_to_gcs(BUCKET_NAME, images, labels, TRAIN_FRACTION, OUTPUT_DIR)\n"
      ],
      "metadata": {
        "id": "uZVs7lrex9Dr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozHkvbb76hv9",
        "outputId": "1c35db20-e841-4d49-9fb6-f1d57004ae5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['p10']\n",
            "['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B10', 'B11']\n"
          ]
        }
      ],
      "source": [
        "# There are 25 strings in the list. \n",
        "# ['p15','p10','p2']\n",
        "# ['p1','p2','p3','p4','p5','p6','p7','p8','p9','p10','p11','p12','p13','p14','p15','p16','p17','p18','p19','p20','p21','p22','p23','p24','p25']\n",
        "RESPONSE =['p10']\n",
        "print(RESPONSE)\n",
        "print(BANDS)\n",
        "\n",
        "\n",
        "# Create a feature stack by concatenating two image collections and converting the result to float.\n",
        "featureStack = ee.Image.cat([\n",
        "  ls_2017.select(BANDS),\n",
        "  wetland_oneHot.select(RESPONSE)\n",
        "]).float()\n",
        "\n",
        "# Define a kernel that will be used for image processing operations. \n",
        "# The kernel is initialized with a list of ones, meaning that each pixel in the kernel will have a weight of one when applied to the image. \n",
        "list = ee.List.repeat(1, KERNEL_SIZE)\n",
        "lists = ee.List.repeat(list, KERNEL_SIZE)\n",
        "kernel = ee.Kernel.fixed(KERNEL_SIZE, KERNEL_SIZE, lists)\n",
        "\n",
        "# Convert the neighborhood values within the kernel to arrays.\n",
        "# Create a new array image where each pixel value is an array representing the neighborhood of the corresponding pixel in the original feature stack. \n",
        "# THe size of the neighborhood is dertermined by the size of the kernel. \n",
        "# 'neighborhoodToArray' is an EE function that converst an image to an array image. \n",
        "arrays = featureStack.neighborhoodToArray(kernel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLV1CArWZWEe",
        "outputId": "83619bb3-3bc2-4a16-e046-f62cda338fb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B10', 'B11', 'p10']\n",
            "{'B1': FixedLenFeature(shape=[128, 128], dtype=tf.float32, default_value=None), 'B2': FixedLenFeature(shape=[128, 128], dtype=tf.float32, default_value=None), 'B3': FixedLenFeature(shape=[128, 128], dtype=tf.float32, default_value=None), 'B4': FixedLenFeature(shape=[128, 128], dtype=tf.float32, default_value=None), 'B5': FixedLenFeature(shape=[128, 128], dtype=tf.float32, default_value=None), 'B6': FixedLenFeature(shape=[128, 128], dtype=tf.float32, default_value=None), 'B7': FixedLenFeature(shape=[128, 128], dtype=tf.float32, default_value=None), 'B10': FixedLenFeature(shape=[128, 128], dtype=tf.float32, default_value=None), 'B11': FixedLenFeature(shape=[128, 128], dtype=tf.float32, default_value=None), 'p10': FixedLenFeature(shape=[128, 128], dtype=tf.float32, default_value=None)}\n"
          ]
        }
      ],
      "source": [
        "# RESPONSE = 'b1'\n",
        "\n",
        "FEATURES = BANDS + RESPONSE\n",
        "print(FEATURES)\n",
        "# Specify the size and shape of patches expected by the model.\n",
        "# KERNEL_SIZE = KERNEL_SIZE\n",
        "KERNEL_SHAPE = [KERNEL_SIZE, KERNEL_SIZE]\n",
        "COLUMNS = [\n",
        "  tf.io.FixedLenFeature(shape=KERNEL_SHAPE, dtype=tf.float32) for k in FEATURES\n",
        "]\n",
        "FEATURES_DICT = dict(zip(FEATURES, COLUMNS))\n",
        "\n",
        "print(FEATURES_DICT)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fD5sj0X-aIXP"
      },
      "outputs": [],
      "source": [
        "# print(featureStack.bandNames().getInfo())\n",
        "\n",
        "# print(featureStack)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zYcnA30aPEj"
      },
      "source": [
        "Use some pre-made geometries to sample the stack in strategic locations. \n",
        "\n",
        "Specifically, these are hand-made polygons in which to take the 128x128 samples. \n",
        "\n",
        "Display the sampling polygons on a map, red for training polygons, blue for evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 491
        },
        "id": "dM41nHujaMvY",
        "outputId": "8e168df4-7714-4839-f5c8-71593febcb6a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<folium.folium.Map at 0x7ff5e10be220>"
            ],
            "text/html": [
              "<div style=\"width:100%;\"><div style=\"position:relative;width:100%;height:0;padding-bottom:60%;\"><span style=\"color:#565656\">Make this Notebook Trusted to load map: File -> Trust Notebook</span><iframe srcdoc=\"&lt;!DOCTYPE html&gt;\n",
              "&lt;html&gt;\n",
              "&lt;head&gt;\n",
              "    \n",
              "    &lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html; charset=UTF-8&quot; /&gt;\n",
              "    \n",
              "        &lt;script&gt;\n",
              "            L_NO_TOUCH = false;\n",
              "            L_DISABLE_3D = false;\n",
              "        &lt;/script&gt;\n",
              "    \n",
              "    &lt;style&gt;html, body {width: 100%;height: 100%;margin: 0;padding: 0;}&lt;/style&gt;\n",
              "    &lt;style&gt;#map {position:absolute;top:0;bottom:0;right:0;left:0;}&lt;/style&gt;\n",
              "    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.js&quot;&gt;&lt;/script&gt;\n",
              "    &lt;script src=&quot;https://code.jquery.com/jquery-1.12.4.min.js&quot;&gt;&lt;/script&gt;\n",
              "    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/js/bootstrap.bundle.min.js&quot;&gt;&lt;/script&gt;\n",
              "    &lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.js&quot;&gt;&lt;/script&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/css/bootstrap.min.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://netdna.bootstrapcdn.com/bootstrap/3.0.0/css/bootstrap.min.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.2.0/css/all.min.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/gh/python-visualization/folium/folium/templates/leaflet.awesome.rotate.min.css&quot;/&gt;\n",
              "    \n",
              "            &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width,\n",
              "                initial-scale=1.0, maximum-scale=1.0, user-scalable=no&quot; /&gt;\n",
              "            &lt;style&gt;\n",
              "                #map_bb3d21948c50deb8203e8f11faa1a0a2 {\n",
              "                    position: relative;\n",
              "                    width: 100.0%;\n",
              "                    height: 100.0%;\n",
              "                    left: 0.0%;\n",
              "                    top: 0.0%;\n",
              "                }\n",
              "                .leaflet-container { font-size: 1rem; }\n",
              "            &lt;/style&gt;\n",
              "        \n",
              "&lt;/head&gt;\n",
              "&lt;body&gt;\n",
              "    \n",
              "    \n",
              "            &lt;div class=&quot;folium-map&quot; id=&quot;map_bb3d21948c50deb8203e8f11faa1a0a2&quot; &gt;&lt;/div&gt;\n",
              "        \n",
              "&lt;/body&gt;\n",
              "&lt;script&gt;\n",
              "    \n",
              "    \n",
              "            var map_bb3d21948c50deb8203e8f11faa1a0a2 = L.map(\n",
              "                &quot;map_bb3d21948c50deb8203e8f11faa1a0a2&quot;,\n",
              "                {\n",
              "                    center: [21.4, -158.0],\n",
              "                    crs: L.CRS.EPSG3857,\n",
              "                    zoom: 4,\n",
              "                    zoomControl: true,\n",
              "                    preferCanvas: false,\n",
              "                }\n",
              "            );\n",
              "\n",
              "            \n",
              "\n",
              "        \n",
              "    \n",
              "            var tile_layer_18e68dd073df827a13cc63a842b5939d = L.tileLayer(\n",
              "                &quot;https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png&quot;,\n",
              "                {&quot;attribution&quot;: &quot;Data by \\u0026copy; \\u003ca target=\\&quot;_blank\\&quot; href=\\&quot;http://openstreetmap.org\\&quot;\\u003eOpenStreetMap\\u003c/a\\u003e, under \\u003ca target=\\&quot;_blank\\&quot; href=\\&quot;http://www.openstreetmap.org/copyright\\&quot;\\u003eODbL\\u003c/a\\u003e.&quot;, &quot;detectRetina&quot;: false, &quot;maxNativeZoom&quot;: 18, &quot;maxZoom&quot;: 18, &quot;minZoom&quot;: 0, &quot;noWrap&quot;: false, &quot;opacity&quot;: 1, &quot;subdomains&quot;: &quot;abc&quot;, &quot;tms&quot;: false}\n",
              "            ).addTo(map_bb3d21948c50deb8203e8f11faa1a0a2);\n",
              "        \n",
              "    \n",
              "            var tile_layer_700163eacf69552c6fa3be5b5c7d92ef = L.tileLayer(\n",
              "                &quot;https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/maps/a03eede290b9576f1540aa598750cd7c-3397b2d26f9a4bdc718a5f048430113b/tiles/{z}/{x}/{y}&quot;,\n",
              "                {&quot;attribution&quot;: &quot;Map Data \\u0026copy; \\u003ca href=\\&quot;https://earthengine.google.com/\\&quot;\\u003eGoogle Earth Engine\\u003c/a\\u003e&quot;, &quot;detectRetina&quot;: false, &quot;maxNativeZoom&quot;: 18, &quot;maxZoom&quot;: 18, &quot;minZoom&quot;: 0, &quot;noWrap&quot;: false, &quot;opacity&quot;: 1, &quot;subdomains&quot;: &quot;abc&quot;, &quot;tms&quot;: false}\n",
              "            ).addTo(map_bb3d21948c50deb8203e8f11faa1a0a2);\n",
              "        \n",
              "    \n",
              "            var layer_control_598570cee33c04cb088bf70ef158f10f = {\n",
              "                base_layers : {\n",
              "                    &quot;openstreetmap&quot; : tile_layer_18e68dd073df827a13cc63a842b5939d,\n",
              "                },\n",
              "                overlays :  {\n",
              "                    &quot;training polygons&quot; : tile_layer_700163eacf69552c6fa3be5b5c7d92ef,\n",
              "                },\n",
              "            };\n",
              "            L.control.layers(\n",
              "                layer_control_598570cee33c04cb088bf70ef158f10f.base_layers,\n",
              "                layer_control_598570cee33c04cb088bf70ef158f10f.overlays,\n",
              "                {&quot;autoZIndex&quot;: true, &quot;collapsed&quot;: true, &quot;position&quot;: &quot;topright&quot;}\n",
              "            ).addTo(map_bb3d21948c50deb8203e8f11faa1a0a2);\n",
              "        \n",
              "&lt;/script&gt;\n",
              "&lt;/html&gt;\" style=\"position:absolute;width:100%;height:100%;left:0;top:0;border:none !important;\" allowfullscreen webkitallowfullscreen mozallowfullscreen></iframe></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ],
      "source": [
        "# FeatureCollection is a colelction of featuers, where each feature is represented as a dictionary of properties and a geometry that describes its spatial extent. \n",
        "# The trainingPolys features in this are used to train the model to recognize land cover types based on the spectral and spatial features extracted from satellite imagery. \n",
        "# The evalPolys features are used to evaluate the accuracy of the trained model. \n",
        "\n",
        "# trainingPolys = ee.FeatureCollection('projects/ee-seismosmsr-landcover/assets/trainingPolys_ccap')\n",
        "# evalPolys = ee.FeatureCollection('projects/ee-seismosmsr-landcover/assets/evalPolys')\n",
        "\n",
        "trainingPolys = ee.FeatureCollection('projects/ee-seismosmsr-landcover/assets/hawaii_not_oahu') \n",
        "evalPolys = ee.FeatureCollection('projects/ee-seismosmsr-landcover/assets/hawaii_oahu')\n",
        "\n",
        "# Create an empty image.\n",
        "# Create a labled image where the pixles belonging to the training set have a value of 1 and evaluation set have a value of 2.\n",
        "polyImage = ee.Image(0).byte().paint(trainingPolys, 1).paint(evalPolys, 2)\n",
        "polyImage = polyImage.updateMask(polyImage)\n",
        "\n",
        "\n",
        "# Use folium to visualize EE image. \n",
        "\n",
        "mapid = polyImage.getMapId({'min': 1, 'max': 2, 'palette': ['red', 'blue']})\n",
        "map = folium.Map(location=[21.4, -158.], zoom_start=4)\n",
        "folium.TileLayer(\n",
        "    tiles=mapid['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='training polygons',\n",
        "  ).add_to(map)\n",
        "map.add_child(folium.LayerControl())\n",
        "map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BpQSamAvhZMx",
        "outputId": "e8832c49-2023-4c25-c708-b40e9914ccd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B10', 'B11', 'p10']\n"
          ]
        }
      ],
      "source": [
        "# Convert the feature collections to lists for iteration.\n",
        "trainingPolysList = trainingPolys.toList(trainingPolys.size())\n",
        "evalPolysList = evalPolys.toList(evalPolys.size())\n",
        "\n",
        "# These numbers determined experimentally.\n",
        "n = 10 # Number of shards in each polygon.\n",
        "N = 200 # Total sample size in each polygon.\n",
        "#200 too big; 100 too big; 50 too big;\n",
        "max_shard_size = 25\n",
        "# Export all the training data (in many pieces), with one task \n",
        "# per geometry.\n",
        "print(BANDS+RESPONSE)\n",
        "# print()\n",
        "for g in range(trainingPolys.size().getInfo()):\n",
        "\n",
        "  for i in range(n):\n",
        "    geomSample = ee.FeatureCollection([])\n",
        "    sample = arrays.sample(\n",
        "      region = ee.Feature(trainingPolysList.get(g)).geometry(), \n",
        "      scale = 30,\n",
        "      numPixels = max_shard_size, # Size of the shard.\n",
        "      seed = i,\n",
        "      tileScale = 8\n",
        "    )\n",
        "    geomSample = geomSample.merge(sample)\n",
        "\n",
        "    desc = TRAINING_BASE + '_g' + str(g) + str(i)\n",
        "    task = ee.batch.Export.table.toCloudStorage(\n",
        "    collection = geomSample,\n",
        "    description = desc,\n",
        "    bucket = BUCKET,\n",
        "    fileNamePrefix = FOLDER + '/' + desc,\n",
        "    fileFormat = 'TFRecord'\n",
        "    # selectors = BANDS + RESPONSE\n",
        "    )\n",
        "    task.start()\n",
        "\n",
        "# Export all the evaluation data.\n",
        "for g in range(evalPolys.size().getInfo()):\n",
        "  geomSample = ee.FeatureCollection([])\n",
        "  for i in range(n):\n",
        "    sample = arrays.sample(\n",
        "      region = ee.Feature(evalPolysList.get(g)).geometry(), \n",
        "      scale = 30,\n",
        "      numPixels = max_shard_size,\n",
        "      seed = i,\n",
        "      tileScale = 8\n",
        "    )\n",
        "    geomSample = geomSample.merge(sample)\n",
        "\n",
        "  desc = EVAL_BASE + '_g' + str(g)\n",
        "  task = ee.batch.Export.table.toCloudStorage(\n",
        "    collection = geomSample,\n",
        "    description = desc,\n",
        "    bucket = BUCKET,\n",
        "    fileNamePrefix = FOLDER + '/' + desc,\n",
        "    fileFormat = 'TFRecord'\n",
        "    # selectors = BANDS + RESPONSE\n",
        "  )\n",
        "  task.start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2WrHB9U5DRT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "360c9882-a46d-4436-f24f-ca1c15de8bd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<function get_dataset at 0x7ff5e18c7790>\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def parse_tfrecord(example_proto):\n",
        "  \"\"\"The parsing function.\n",
        "  Read a serialized example into the structure defined by FEATURES_DICT.\n",
        "  Args:\n",
        "    example_proto: a serialized Example.\n",
        "  Returns:\n",
        "    A dictionary of tensors, keyed by feature name.\n",
        "  \"\"\"\n",
        "  return tf.io.parse_single_example(example_proto, FEATURES_DICT)\n",
        "\n",
        "\n",
        "def to_tuple(inputs):\n",
        "  \"\"\"Function to convert a dictionary of tensors to a tuple of (inputs, outputs).\n",
        "  Turn the tensors returned by parse_tfrecord into a stack in HWC shape.\n",
        "  Args:\n",
        "    inputs: A dictionary of tensors, keyed by feature name.\n",
        "  Returns:\n",
        "    A tuple of (inputs, outputs).\n",
        "  \"\"\"\n",
        "  inputsList = [inputs.get(key) for key in FEATURES]\n",
        "  stacked = tf.stack(inputsList, axis=0)\n",
        "  # Convert from CHW to HWC\n",
        "  stacked = tf.transpose(stacked, [1, 2, 0])\n",
        "  return stacked[:,:,:len(BANDS)], stacked[:,:,len(BANDS):]\n",
        "\n",
        "\n",
        "def get_dataset(pattern):\n",
        "  \"\"\"Function to read, parse and format to tuple a set of input tfrecord files.\n",
        "  Get all the files matching the pattern, parse and convert to tuple.\n",
        "  Args:\n",
        "    pattern: A file pattern to match in a Cloud Storage bucket.\n",
        "  Returns:\n",
        "    A tf.data.Dataset\n",
        "  \"\"\"\n",
        "  glob = tf.io.gfile.glob(pattern)\n",
        "  dataset = tf.data.TFRecordDataset(glob, compression_type='GZIP')\n",
        "  dataset = dataset.map(parse_tfrecord, num_parallel_calls=5)\n",
        "  dataset = dataset.map(to_tuple, num_parallel_calls=5)\n",
        "  return dataset\n",
        "\n",
        "\n",
        "print(get_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PYE9NP8j5OWN"
      },
      "outputs": [],
      "source": [
        "# Define a function that returns a preprocessed dataset for ML. \n",
        "# Match the training data files in GCS bucket.\n",
        "# Calls 'get_dataset' function to read, parse, and convert the input files to tuples of tensors. \n",
        "# The resulting dataset is shuffled, batched, and repeated (epochs) to prepare for ML training. \n",
        "# Save the resulting dataset to 'training' variable. \n",
        "def get_training_dataset():\n",
        "\t\"\"\"Get the preprocessed training dataset\n",
        "  Returns: \n",
        "    A tf.data.Dataset of training data.\n",
        "  \"\"\"\n",
        "\tglob = 'gs://' + BUCKET + '/' + FOLDER + '/' + TRAINING_BASE + '*'\n",
        "\t# print(glob)\n",
        "\tdataset = get_dataset(glob)\n",
        "\tdataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
        "\treturn dataset\n",
        "\n",
        "training = get_training_dataset()\n",
        "\n",
        "# print(iter(training.take(2)).next())\n",
        "# This appears to be a tensor with shape (16, 128, 128, 9) and data type float32. \n",
        "# The first dimension has a size of 16, which suggests that this tensor contains a batch of 16 samples. \n",
        "# The next two dimensions have a size of 128, which suggests that each sample has an image with a resolution of 128 x 128 pixels. \n",
        "# The last dimension has a size of 9, which suggests that each pixel is represented by a 9-dimensional feature vector. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNNJCqsJ5RBx",
        "outputId": "b37f9591-18ca-4c51-cbca-8dbe406f65ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gs://remote_sensing_fuckit_bucket/wetland_unet/eval_patches*\n"
          ]
        }
      ],
      "source": [
        "# Return a Tenserflow dataset contaiing the preprocessed evaluation data. \n",
        "# File path to the evaluation data stored in GCS bucket.\n",
        "# Calls 'get_dataset' function to create a TenserFlow dataset containing the evaluation data. \n",
        "# Evaluation dataset is batched with a batchsize of 1, meaning each element of the dataset consists of a single example. \n",
        "# The function returns the resulting evaluation dataset.\n",
        "def get_eval_dataset():\n",
        "\t\"\"\"Get the preprocessed evaluation dataset\n",
        "  Returns: \n",
        "    A tf.data.Dataset of evaluation data.\n",
        "  \"\"\"\n",
        "\tglob = 'gs://' + BUCKET + '/' + FOLDER + '/' + EVAL_BASE + '*'\n",
        "\tprint(glob)\n",
        "\tdataset = get_dataset(glob)\n",
        "\tdataset = dataset.batch(1).repeat()\n",
        "\treturn dataset\n",
        "\n",
        "evaluation = get_eval_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjIctHU75YAB",
        "outputId": "aac06664-dc46-4f01-dd0b-1c5606cc3510"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_RepeatDataset element_spec=(TensorSpec(shape=(None, 128, 128, 9), dtype=tf.float32, name=None), TensorSpec(shape=(None, 128, 128, 1), dtype=tf.float32, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ],
      "source": [
        "training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nndrupcf5vFi",
        "outputId": "511335fc-42d6-4540-fa57-989bd7f9acbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.12.0\n"
          ]
        }
      ],
      "source": [
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPe--dX05zZX"
      },
      "outputs": [],
      "source": [
        "# import TenserFlow classes and functions \n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import metrics\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "\n",
        "# from tensorflow.python.keras.layers.normalization import BatchNormalization\n",
        "# from tensorflow.python.keras.layers.normalization import BatchNormalization\n",
        "\n",
        "# from tensorflow.keras.layers import BatchNormalization\n",
        "# from keras.layers.normalization.batch_normalization import BatchNormalization\n",
        "# from tensorflow.python.keras.layers import BatchNormalization \n",
        "\n",
        "# U-Net model for image segmentation. \n",
        "# Encoder and decoder conncted by a center block. \n",
        "# Encoder downsamples the input image while capturing its features. \n",
        "# Decoder upsamples the encoded image to generate a segmentation map.\n",
        "def conv_block(input_tensor, num_filters):\n",
        "\tencoder = layers.Conv2D(num_filters, (3, 3), padding='same')(input_tensor)\n",
        "\tencoder = layers.BatchNormalization()(encoder)\n",
        "\tencoder = layers.Activation('relu')(encoder)\n",
        "\tencoder = layers.Conv2D(num_filters, (3, 3), padding='same')(encoder)\n",
        "\tencoder = layers.BatchNormalization()(encoder)\n",
        "\tencoder = layers.Activation('relu')(encoder)\n",
        "\treturn encoder\n",
        "\n",
        "def encoder_block(input_tensor, num_filters):\n",
        "\tencoder = conv_block(input_tensor, num_filters)\n",
        "\tencoder_pool = layers.MaxPooling2D((2, 2), strides=(2, 2))(encoder)\n",
        "\treturn encoder_pool, encoder\n",
        "\n",
        "def decoder_block(input_tensor, concat_tensor, num_filters):\n",
        "\tdecoder = layers.Conv2DTranspose(num_filters, (2, 2), strides=(2, 2), padding='same')(input_tensor)\n",
        "\tdecoder = layers.concatenate([concat_tensor, decoder], axis=-1)\n",
        "\tdecoder = layers.BatchNormalization()(decoder)\n",
        "\tdecoder = layers.Activation('relu')(decoder)\n",
        "\tdecoder = layers.Conv2D(num_filters, (3, 3), padding='same')(decoder)\n",
        "\tdecoder = layers.BatchNormalization()(decoder)\n",
        "\tdecoder = layers.Activation('relu')(decoder)\n",
        "\tdecoder = layers.Conv2D(num_filters, (3, 3), padding='same')(decoder)\n",
        "\tdecoder = layers.BatchNormalization()(decoder)\n",
        "\tdecoder = layers.Activation('relu')(decoder)\n",
        "\treturn decoder\n",
        "\n",
        "def get_model():\n",
        "\tinputs = layers.Input(shape=[KERNEL_SIZE, KERNEL_SIZE, len(BANDS)]) # 256\n",
        "\tencoder0_pool, encoder0 = encoder_block(inputs, 32) # 128\n",
        "\tencoder1_pool, encoder1 = encoder_block(encoder0_pool, 64) # 64\n",
        "\tencoder2_pool, encoder2 = encoder_block(encoder1_pool, 128) # 32\n",
        "\tencoder3_pool, encoder3 = encoder_block(encoder2_pool, 256) # 16\n",
        "\tencoder4_pool, encoder4 = encoder_block(encoder3_pool, 512) # 8\n",
        "\tcenter = conv_block(encoder4_pool, 1024) # center\n",
        "\tdecoder4 = decoder_block(center, encoder4, 512) # 16\n",
        "\tdecoder3 = decoder_block(decoder4, encoder3, 256) # 32\n",
        "\tdecoder2 = decoder_block(decoder3, encoder2, 128) # 64\n",
        "\tdecoder1 = decoder_block(decoder2, encoder1, 64) # 128\n",
        "\tdecoder0 = decoder_block(decoder1, encoder0, 32) # 256\n",
        "\toutputs = layers.Conv2D(len(RESPONSE), (1, 1), activation='softmax')(decoder0)\n",
        " \n",
        "def get_model():\n",
        "\tinputs = layers.Input(shape=[KERNEL_SIZE, KERNEL_SIZE, len(BANDS)]) # 256\n",
        "\tencoder0_pool, encoder0 = encoder_block(inputs, 32) # 128\n",
        "\t# encoder1_pool, encoder1 = encoder_block(encoder0_pool, 64) # 64\n",
        "\t# encoder2_pool, encoder2 = encoder_block(encoder1_pool, 128) # 32\n",
        "\t# encoder3_pool, encoder3 = encoder_block(encoder2_pool, 256) # 16\n",
        "\t# encoder4_pool, encoder4 = encoder_block(encoder3_pool, 512) # 8\n",
        "\tcenter = conv_block(encoder0_pool, 64) # center\n",
        "\t# decoder4 = decoder_block(center, encoder4, 512) # 16\n",
        "\t# decoder3 = decoder_block(decoder4, encoder3, 256) # 32\n",
        "\t# decoder2 = decoder_block(decoder3, encoder2, 128) # 64\n",
        "\t# decoder1 = decoder_block(center, encoder1, 64) # 128\n",
        "\tdecoder0 = decoder_block(center, encoder0, 32) # 256\n",
        "\toutputs = layers.Conv2D(len(RESPONSE), (1, 1), activation='softmax')(decoder0)\n",
        "\t\n",
        "\tmodel = models.Model(inputs=[inputs], outputs=[outputs])\n",
        "\n",
        "\tmodel.compile(\n",
        "\t\t# optimizer=optimizers.get(OPTIMIZER),\n",
        "\t\t# optimizer=tf.keras.losses.SparseCategoricalCrossentropy(),  \n",
        "\t\tloss=losses.get(LOSS),\n",
        "\t\toptimizer= tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
        "\t\t# loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "\t\tmetrics=[metrics.get(metric) for metric in METRICS])\n",
        "\n",
        "\treturn model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LOSS = 'BinaryCrossentropy'"
      ],
      "metadata": {
        "id": "LYyP654cV36W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(LOSS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rBJKBJCVPGZ",
        "outputId": "a1790ff7-dfe9-421e-b45e-31533483c6d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BinaryCrossentropy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXK-0qM42UwP",
        "outputId": "0a44950b-8fc7-4e75-dfbc-9dd449988b43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_18\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_20 (InputLayer)          [(None, 128, 128, 9  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv2d_208 (Conv2D)            (None, 128, 128, 32  2624        ['input_20[0][0]']               \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_228 (Batch  (None, 128, 128, 32  128        ['conv2d_208[0][0]']             \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " activation_228 (Activation)    (None, 128, 128, 32  0           ['batch_normalization_228[0][0]']\n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_209 (Conv2D)            (None, 128, 128, 32  9248        ['activation_228[0][0]']         \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_229 (Batch  (None, 128, 128, 32  128        ['conv2d_209[0][0]']             \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " activation_229 (Activation)    (None, 128, 128, 32  0           ['batch_normalization_229[0][0]']\n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " max_pooling2d_39 (MaxPooling2D  (None, 64, 64, 32)  0           ['activation_229[0][0]']         \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv2d_210 (Conv2D)            (None, 64, 64, 64)   18496       ['max_pooling2d_39[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_230 (Batch  (None, 64, 64, 64)  256         ['conv2d_210[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_230 (Activation)    (None, 64, 64, 64)   0           ['batch_normalization_230[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_211 (Conv2D)            (None, 64, 64, 64)   36928       ['activation_230[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_231 (Batch  (None, 64, 64, 64)  256         ['conv2d_211[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_231 (Activation)    (None, 64, 64, 64)   0           ['batch_normalization_231[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_transpose_38 (Conv2DTra  (None, 128, 128, 32  8224       ['activation_231[0][0]']         \n",
            " nspose)                        )                                                                 \n",
            "                                                                                                  \n",
            " concatenate_38 (Concatenate)   (None, 128, 128, 64  0           ['activation_229[0][0]',         \n",
            "                                )                                 'conv2d_transpose_38[0][0]']    \n",
            "                                                                                                  \n",
            " batch_normalization_232 (Batch  (None, 128, 128, 64  256        ['concatenate_38[0][0]']         \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " activation_232 (Activation)    (None, 128, 128, 64  0           ['batch_normalization_232[0][0]']\n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_212 (Conv2D)            (None, 128, 128, 32  18464       ['activation_232[0][0]']         \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_233 (Batch  (None, 128, 128, 32  128        ['conv2d_212[0][0]']             \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " activation_233 (Activation)    (None, 128, 128, 32  0           ['batch_normalization_233[0][0]']\n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_213 (Conv2D)            (None, 128, 128, 32  9248        ['activation_233[0][0]']         \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_234 (Batch  (None, 128, 128, 32  128        ['conv2d_213[0][0]']             \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " activation_234 (Activation)    (None, 128, 128, 32  0           ['batch_normalization_234[0][0]']\n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_214 (Conv2D)            (None, 128, 128, 1)  33          ['activation_234[0][0]']         \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 104,545\n",
            "Trainable params: 103,905\n",
            "Non-trainable params: 640\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "m = get_model()\n",
        "print(m.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G39jZYYX6B0E",
        "outputId": "f8a13576-5dea-4cf0-f935-82d5c00f9cec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "50/50 [==============================] - 222s 491ms/step - loss: 0.6755 - accuracy: 0.2493 - val_loss: 0.6991 - val_accuracy: 0.3906\n",
            "Epoch 2/50\n",
            "50/50 [==============================] - 51s 1s/step - loss: 0.6352 - accuracy: 0.2472 - val_loss: 0.7057 - val_accuracy: 0.3906\n",
            "Epoch 3/50\n",
            "50/50 [==============================] - 23s 473ms/step - loss: 0.6093 - accuracy: 0.2515 - val_loss: 0.7087 - val_accuracy: 0.3906\n",
            "Epoch 4/50\n",
            "50/50 [==============================] - 61s 1s/step - loss: 0.5816 - accuracy: 0.2455 - val_loss: 0.7038 - val_accuracy: 0.3906\n",
            "Epoch 5/50\n",
            "50/50 [==============================] - 16s 319ms/step - loss: 0.5603 - accuracy: 0.2529 - val_loss: 0.6909 - val_accuracy: 0.3906\n",
            "Epoch 6/50\n",
            "50/50 [==============================] - 52s 1s/step - loss: 0.5495 - accuracy: 0.2433 - val_loss: 0.6731 - val_accuracy: 0.3906\n",
            "Epoch 7/50\n",
            "50/50 [==============================] - 14s 286ms/step - loss: 0.5337 - accuracy: 0.2458 - val_loss: 0.6549 - val_accuracy: 0.3906\n",
            "Epoch 8/50\n",
            "50/50 [==============================] - 59s 1s/step - loss: 0.5279 - accuracy: 0.2506 - val_loss: 0.6354 - val_accuracy: 0.3906\n",
            "Epoch 9/50\n",
            "50/50 [==============================] - 15s 303ms/step - loss: 0.5169 - accuracy: 0.2453 - val_loss: 0.6179 - val_accuracy: 0.3906\n",
            "Epoch 10/50\n",
            "50/50 [==============================] - 61s 1s/step - loss: 0.5054 - accuracy: 0.2639 - val_loss: 0.5997 - val_accuracy: 0.3906\n",
            "Epoch 11/50\n",
            "50/50 [==============================] - 23s 475ms/step - loss: 0.4968 - accuracy: 0.2460 - val_loss: 0.5845 - val_accuracy: 0.3906\n",
            "Epoch 12/50\n",
            "50/50 [==============================] - 60s 1s/step - loss: 0.4828 - accuracy: 0.2404 - val_loss: 0.5761 - val_accuracy: 0.3906\n",
            "Epoch 13/50\n",
            "50/50 [==============================] - 15s 297ms/step - loss: 0.4813 - accuracy: 0.2540 - val_loss: 0.5648 - val_accuracy: 0.3906\n",
            "Epoch 14/50\n",
            "50/50 [==============================] - 44s 893ms/step - loss: 0.4808 - accuracy: 0.2406 - val_loss: 0.5666 - val_accuracy: 0.3906\n",
            "Epoch 15/50\n",
            "50/50 [==============================] - 23s 473ms/step - loss: 0.4763 - accuracy: 0.2515 - val_loss: 0.5621 - val_accuracy: 0.3906\n",
            "Epoch 16/50\n",
            "50/50 [==============================] - 51s 1s/step - loss: 0.4670 - accuracy: 0.2468 - val_loss: 0.5654 - val_accuracy: 0.3906\n",
            "Epoch 17/50\n",
            "50/50 [==============================] - 16s 316ms/step - loss: 0.4685 - accuracy: 0.2520 - val_loss: 0.5607 - val_accuracy: 0.3906\n",
            "Epoch 18/50\n",
            "50/50 [==============================] - 40s 809ms/step - loss: 0.4572 - accuracy: 0.2492 - val_loss: 0.5591 - val_accuracy: 0.3906\n",
            "Epoch 19/50\n",
            "50/50 [==============================] - 16s 319ms/step - loss: 0.4594 - accuracy: 0.2489 - val_loss: 0.5593 - val_accuracy: 0.3906\n",
            "Epoch 20/50\n",
            "30/50 [=================>............] - ETA: 16s - loss: 0.4679 - accuracy: 0.2445"
          ]
        }
      ],
      "source": [
        "m = get_model()\n",
        "# tf.keras.utils.plot_model(m)\n",
        "# Load a trained model. 50 epochs. 25 hours. Final RMSE ~0.08.\n",
        "MODEL_DIR = 'gs://' + BUCKET +'/'+ FOLDER +  '/'\n",
        "\n",
        "# m = tf.keras.models.load_model(MODEL_DIR)\n",
        "# EPOCHS\n",
        "#int(TRAIN_SIZE / BATCH_SIZE)\n",
        "m.fit(\n",
        "    x=training, \n",
        "    epochs=50, \n",
        "    steps_per_epoch=50, \n",
        "    validation_data=evaluation,\n",
        "    validation_steps=EVAL_SIZE)\n",
        "\n",
        "m.save('gs://' + BUCKET +'/'+ FOLDER +  '/')\n",
        "print(m)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6LYDKybxt0qI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def doExport(out_image_base, shape, region):\n",
        "  \"\"\"Run the image export task.  Block until complete.\n",
        "  \"\"\"\n",
        "  task = ee.batch.Export.image.toDrive(\n",
        "    image = ls_2017.select(BANDS),\n",
        "    description = out_image_base,\n",
        "    fileNamePrefix = out_image_base,\n",
        "    folder = FOLDER,\n",
        "    region = region.getInfo()['coordinates'],\n",
        "    scale = 30,\n",
        "    fileFormat = 'TFRecord',\n",
        "    maxPixels = 1e10,\n",
        "    formatOptions = {\n",
        "      'patchDimensions': shape,\n",
        "      'compressed': True,\n",
        "      'maxFileSize': 104857600\n",
        "    }\n",
        "  )\n",
        "  task.start()\n",
        "\n",
        "  # Block until the task completes.\n",
        "  print('Running image export to Google Drive...')\n",
        "  import time\n",
        "  while task.active():\n",
        "    time.sleep(30)\n",
        "\n",
        "  # Error condition\n",
        "  if task.status()['state'] != 'COMPLETED':\n",
        "    print('Error with image export.')\n",
        "  else:\n",
        "    print('Image export completed.')"
      ],
      "metadata": {
        "id": "Ld7W6gm80tv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def doPrediction(out_image_base, kernel_shape, region):\n",
        "  \"\"\"Perform inference on exported imagery.\n",
        "  \"\"\"\n",
        "\n",
        "  print('Looking for TFRecord files...')\n",
        "\n",
        "  # Get a list of all the files in the output bucket.\n",
        "  filesList = os.listdir(op.join(ROOT_DIR, FOLDER))\n",
        "\n",
        "  # Get only the files generated by the image export.\n",
        "  exportFilesList = [s for s in filesList if out_image_base in s]\n",
        "\n",
        "  # Get the list of image files and the JSON mixer file.\n",
        "  imageFilesList = []\n",
        "  jsonFile = None\n",
        "  for f in exportFilesList:\n",
        "    if f.endswith('.tfrecord.gz'):\n",
        "      imageFilesList.append(op.join(ROOT_DIR, FOLDER, f))\n",
        "    elif f.endswith('.json'):\n",
        "      jsonFile = f\n",
        "\n",
        "  # Make sure the files are in the right order.\n",
        "  imageFilesList.sort()\n",
        "\n",
        "  from pprint import pprint\n",
        "  pprint(imageFilesList)\n",
        "  print(jsonFile)\n",
        "\n",
        "  import json\n",
        "  # Load the contents of the mixer file to a JSON object.\n",
        "  with open(op.join(ROOT_DIR, FOLDER, jsonFile), 'r') as f:\n",
        "    mixer = json.load(f)\n",
        "\n",
        "  pprint(mixer)\n",
        "  patches = mixer['totalPatches']\n",
        "\n",
        "  # Get set up for prediction.\n",
        "\n",
        "  imageColumns = [\n",
        "    tf.io.FixedLenFeature(shape=kernel_shape, dtype=tf.float32) \n",
        "      for k in BANDS\n",
        "  ]\n",
        "\n",
        "  imageFeaturesDict = dict(zip(BANDS, imageColumns))\n",
        "\n",
        "  def parse_image(example_proto):\n",
        "    return tf.io.parse_single_example(example_proto, imageFeaturesDict)\n",
        "\n",
        "  def toTupleImage(inputs):\n",
        "    inputsList = [inputs.get(key) for key in BANDS]\n",
        "    stacked = tf.stack(inputsList, axis=0)\n",
        "    stacked = tf.transpose(stacked, [1, 2, 0])\n",
        "    return stacked\n",
        "\n",
        "   # Create a dataset from the TFRecord file(s) in Cloud Storage.\n",
        "  imageDataset = tf.data.TFRecordDataset(imageFilesList, compression_type='GZIP')\n",
        "  imageDataset = imageDataset.map(parse_image, num_parallel_calls=5)\n",
        "  imageDataset = imageDataset.map(toTupleImage).batch(1)\n",
        "\n",
        "  # Perform inference.\n",
        "  print('Running predictions...')\n",
        "  predictions = model.predict(imageDataset, steps=patches, verbose=1)\n",
        "  # print(predictions[0])\n",
        "\n",
        "  print('Writing predictions...')\n",
        "  out_image_file = op.join(ROOT_DIR, FOLDER, f'{out_image_base}pred.TFRecord')\n",
        "  writer = tf.io.TFRecordWriter(out_image_file)\n",
        "  patches = 0\n",
        "  for predictionPatch in predictions:\n",
        "    print('Writing patch ' + str(patches) + '...')\n",
        "    predictionPatch = tf.argmax(predictionPatch, axis=2)\n",
        "\n",
        "    # Create an example.\n",
        "    example = tf.train.Example(\n",
        "      features=tf.train.Features(\n",
        "        feature={\n",
        "          'class': tf.train.Feature(\n",
        "              float_list=tf.train.FloatList(\n",
        "                  value=predictionPatch.numpy().flatten()))\n",
        "        }\n",
        "      )\n",
        "    )\n",
        "    # Write the example.\n",
        "    writer.write(example.SerializeToString())\n",
        "    patches += 1\n",
        "\n",
        "  writer.close()"
      ],
      "metadata": {
        "id": "AMwcooPI02OX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Output assets folder: YOUR FOLDER\n",
        "user_folder = 'users/seismosmsr/UNET_regression' # INSERT YOUR FOLDER HERE.\n",
        "\n",
        "# Base file name to use for TFRecord files and assets.\n",
        "image_base = 'FCNN_demo_Oahu'\n",
        "# Half this will extend on the sides of each patch.\n",
        "KERNEL_SHAPE = [128, 128]\n",
        "# Beijing\n",
        "#-123.3152617122404422,37.7933818681806812 : -121.5833326464016722,38.6741999222186763\n",
        "OahuDistricts = ee.FeatureCollection('projects/ee-seismosmsr-landcover/assets/Neighborhood_Board_Subdistricts')\n",
        "query_str = f'SD_DESC == \"Makaha\"'\n",
        "makaha = OahuDistricts.filter(query_str).geometry() \n",
        "# makaha.getInfo()"
      ],
      "metadata": {
        "id": "jK6xOeE-3UYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doExport(image_base, KERNEL_SHAPE,makaha)"
      ],
      "metadata": {
        "id": "domZ7b_13cbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount our Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "2rO3R07C3enC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os as os\n",
        "from os import path as op\n",
        "model = m\n",
        "ROOT_DIR = '/content/drive/My Drive/'\n",
        "print(image_base)\n",
        "doPrediction(image_base, KERNEL_SHAPE, makaha)"
      ],
      "metadata": {
        "id": "ONh2ltTY3kFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out_image = ee.Image('projects/ee-seismosmsr-landcover/assets/FCNN_demo_Oahu-mixer')\n",
        "mapid = out_image.getMapId({'min': 0, 'max': 25, 'palette': ['63C600','E6E600','E9BD3A','ECB176','00A600','63C600','E6E600','E9BD3A','ECB176','00A600','63C600','E6E600','E9BD3A','ECB176','00A600','63C600','E6E600','E9BD3A','ECB176','00A600','63C600','E6E600','E9BD3A','ECB176','00A600','63C600','E6E600','E9BD3A','ECB176']})\n",
        "map = folium.Map(location=[21.4, -158])\n",
        "# map = folium.Map(location=[              \n",
        "#               -29.177943749121233,\n",
        "#               30.55984497070313,\n",
        "# ])\n",
        "folium.TileLayer(\n",
        "    tiles=mapid['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='predicted crop type',\n",
        "  ).add_to(map)\n",
        "map.add_child(folium.LayerControl())\n",
        "map"
      ],
      "metadata": {
        "id": "A2oItq-T3llR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzM45ZQNvVnH"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}